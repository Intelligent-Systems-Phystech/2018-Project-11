\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
\usepackage{footnote}
\usepackage{jmlda}


\begin{document}

\title
    {Автоматическое построение нейросети оптимальной сложности}
\author
    {Горян$^1$~Н.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление

\organization
    {$^1$Московский физико-технический институт\par
    $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\email
    {goryan.na@phystech.edu; bakhteev@phystech.edu; strijov@phystech.edu}    


    

\abstract
	{Работа посвящена выбору оптимальной модели нейронной сети. Нейронная сеть рассматривается как вычислительный граф, рёбра которого --- примитивные функции, а вершины --- промежуточные представления выборки. Предполагается, что структуру нейронной сети можно упростить без значимой потери качества классификации. Структура нейросети опеределяется вершинами симплекса. Для определения нужной структуры нейронной сети предлагается проводить оптимизацию гиперпараметров и структурных параметров. Для решения задачи оптимизации предлагается проводить релаксацию структуры. Для анализа качества представленного алгоритма проводятся эксперименты на выборках Boston, MNIST и CIFAR-10.
\bigskip

\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, прореживание нейронной сети, оптимальная структура нейронной сети, вариационный вывод}.
}


\maketitle


\section{ Введение}
	
	В данной работе рассматривается алгоритм построения оптимальной нейронной сети. Одной из основных областей применения являются мобильные устройства, которые в силу своих ограниченных вычислительных ресурсов не могут справляться с избыточно сложными неросетями~\cite{rallapalli2016very}. Существует ряд способов построения нейронных сетей. Для того, чтобы подобрать нужную сеть требуется определить оптимальные значения структурных параметров~\cite{Myung1997}: количество слоёв, нейронов в каждом слое и функции активации каждого нейрона. Выбор этих гиперпараметров является вычислительно сложной задачей~\cite{sutskever2014}.
	
	В работах~\cite{cun1990, graves2011} используется алгоритм прореживания нейросети. Он заключается в построении заведомо переусложнённой модели, которая в последствии упрощается. Ещё одиним способом, предложенным в работе~\cite{Maclaurin:2015:GHO:3045118.3045343}, является метаобучение, которое получая на вход некоторую выборку возвращает оптимальные гиперпараметры.
	В данной работе исследуется алгоритм, который оптимизирует параметры, гиперпараметры, структурные параметры нейросети в единой процедуре. В основе лежит алгоритм DARTS, предложенным в работе~\cite{liu2018darts}, в основе которого лежит процедура релаксации: переход от дискретного множества структурных параметров к непрерывному, что позволяет использовать методы градиентной оптимизации для нахождения лучших гиперпараметров. Входными данными алгоритма являются некоторый набор данных и заранее определённый набор функций активации. Как результат мы получаем оптимальную нейросеть.
	
	Проверка и анализ метода проводится на выборке Boston Housing~\cite{Boston}, MNIST~\cite{MNIST},  CIFAR-10~\cite{CIFAR-10} и синтетических данных. Полученная модель сравнивается с моделями, полученными при помощи базовых алгоритмов.

\section{Постановка задачи}
	Пусть заданы обучающая и валидационная выборки
	$$\mathfrak{D}^{train} = \{\textbf{x}_i,y_i\},~ i =1,...,m^{train},\eqno(?)$$
	$$\mathfrak{D}^{valid} = \{\textbf{x}_i,y_i\},~ i =1,...,m^{valid}, \eqno(?)$$
	где $\mathbf{x}_i \in \mathbb{R}^n$  --- объекты, а $y_{i} \in \{1,..., Z \}$ --- метки объектов $\mathbf{x}_i$, где $Z$ --- количество классов.\\
	Модель описывается ориентированным графом $(V, E)$,  для каждого ребра которого $(j, k) \in E$ определён вектор базовых функций $\textbf{g}_{j, k}$ мощностью $K_{j, k}$. Модель $\textbf{f}(\textbf{x}, \textbf{W})$ задаётся параметрами подмоделей $\{f_{v}\}_{v = 1}^{|V|}$ и структурными параметрами $\gamma$.  
	Каждая подмодель $f_{v}$ описывается через графовое представление модели:
	
	 $$f_{v}(\textbf{x}) = \sum\limits_{k \in Adj(v_i)} <\gamma_{j, k}, g_{j, k}>f_{k}(\textbf{x}, \textbf{w}), \quad f_{0}(\textbf{x}) = \textbf{x}.$$ 
	Параметры модели $\textbf{W}$ --- конкатенация параметров всех подмоделей $\{f_{v}\}_{v = 1}^{|V|}$.\\
	Структура модели $\textbf{Г}$ --- конкатенация структурных параметров $\gamma$.\\
	\\
	Функции потерь на обучении $L$ и валидации  $Q$ задаются:
	$$L =  \text{log} p(\mathfrak{D}^{train}, \textbf{W}, \textbf{Г})+ e^{\textbf{A}}||\textbf{W}||,$$ 
	$$Q =  \text{log} p(\mathfrak{D}^{valid}, \textbf{W}, \textbf{Г}). $$
	Таким образом, получили задачу двухуровневой оптимизации:
	$$\textbf{W}^{*} = \argmin_{ \textbf{W}}L(\mathfrak{D}^{train}, \textbf{W}, \textbf{Г}, \textbf{A})$$
	$$\textbf{Г}^{*} , \textbf{A}^{*} = \argmin_{ \textbf{Г}, \textbf{A}}Q(\mathfrak{D}^{valid}, \textbf{W}, \textbf{Г}, \textbf{A}).$$
	
\paragraph{Релаксация модели}\\
Пусть у нас есть множество кандидатов на роль базовой функции мощности $\mathbb{K}$. Каждая базовая функция применима к нейрону $x^{i}$ сети. Для того чтобы перейти от дискретной задачи поиска оптимальной базовой функции к непрерывной, производим релаксацию структуры модели используя softmax:\\
$$\overline{g}^{(i, j)}(x) = \sum\limits_{g \in \mathbb{K}}{\frac{exp(\gamma_{g}^{(i, j)})}{\sum\limits_{\overline{g} \in \mathbb{K}}exp(\gamma_{\overline{g}}^{(i, j)})}g(x)},$$\\
 где $\gamma^{(i, j)}$ --- вектор размерности $|\mathbb{K}|$, парамметризующий комбинацию базовых функций. Таким образом мы перешли к задаче поиска базовой функции, подбирая непрерывные параметры  $\gamma$. В конце поиска, каждая комбинация базовых функций $\overline{g}^{(i, j)}(x) $  меняется на $g^{(i, j)} = \argmax\limits_{g \in \mathbb{K}}\gamma_{g}^{(i, j)}$.\\

 \paragraph{Регуляризация структуры модели}\\
 Регуляризация структуры проводится добавлением к фунции потерь $Q$ слагаемого $\lambda P(\Gamma)$, где $P(\Gamma)$ есть произведение всех вероятностей возникновения веса $\gamma^{(i, j)}$.\\
 В качестве вероятности для структуры можно использовать Gumble-Softmax или распределение Дирихле.\\
 
 \paragraph{ Оптимизация гиперпараметров и структурных параметров модели}\\
Потери на валидационной и обучающей выборке обусловлены как структурой модели Г так и параметрами модели W. Цель поиска архитектуры найти модель $\text{Г}^{*}$, которая минимизирует ошибку на валидационной выборке  $Q(w^{*}, \text{Г}^{*})$. При этом $w^{*}$ находится из условия минимизации функции потерь $L(w, \text{Г}^{*}).$ \\
Таким образом получается задача двухуровневой оптимизации:
$$\min\limits_{\text{Г}} \quad L(w^{*}(\text{Г}), \text{Г})$$
$$s.t. \quad w^{*}(\text{Г}) = \argmin_{w}Q(w, \text{Г})$$.\\
Чтобы решить эту задачу мы используем итеративную оптимизационную процедуру, в которой Г и $w$ оптимизируются по очереди с помощью градиентного спуска. На $k$-м шаге, имея структуру модели $\text{Г}_{k-1}$, полуачем $w_{k}$ изменяя $w_{k-1}$ в сторону минимизации $L(w_{k-1}, \text{Г}_{k-1}).$ Далее, фиксируя $w_{k}$, находим $\text{Г}_{k}$, минимизируя $Q(w_{k-1} - \xi \nabla_{w}L(w_{k-1}, \text{Г}_{k-1}), \text{Г}_{k-1}), $  где $\xi$ шаг градиентного спуска.

 
 
	
  
\bibliographystyle{unsrt}
\bibliography{Goryan2018Project11}
\end{document}
