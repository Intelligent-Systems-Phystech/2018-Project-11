\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
\usepackage{footnote}
\usepackage{jmlda}


\begin{document}

\title
    {Автоматическое построение нейросети оптимальной сложности}
\author
    {Горян$^1$~Н.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление

\organization
    {$^1$Московский физико-технический институт\par
    $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\email
    {goryan.na@phystech.edu; bakhteev@phystech.edu; strijov@phystech.edu}    


    

\abstract
	{Работа посвящена выбору оптимальной модели нейронной сети. Нейронная сеть рассматривается как вычислительный граф, рёбра которого --- примитивные функции, а вершины --- промежуточные представления выборки. Предполагается, что структуру нейронной сети можно упростить без значимой потери качества классификации. Структура нейросети опеределяется вершинами симплекса. Для определения нужной структуры нейронной сети предлагается проводить оптимизацию гиперпараметров и структурных параметров. Для решения задачи оптимизации предлагается проводить релаксацию структуры. Для анализа качества представленного алгоритма проводятся эксперименты на выборках Boston, MNIST и CIFAR-10.
\bigskip

\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, прореживание нейронной сети, оптимальная структура нейронной сети, вариационный вывод}.
}


\maketitle


\section{ Введение}
	
	В данной работе рассматривается метод построения оптимальной нейронной сети. Одной из основных областей применения являются мобильные устройства, которые в силу своих ограниченных вычислительных ресурсов не могут справляться с избыточно сложными неросетями~\cite{rallapalli2016very}. Существует ряд способов построения нейронных сетей. Для того, чтобы выбрать нужную сеть требуется узнать её гиперпараметры~\cite{Myung1997}: количество слоёв, нейронов в каждом слое и функции активации каждого нейрона. Подбор этих гиперпараметров является вычислительно сложной задачей~\cite{sutskever2014}.
	
	В работах~\cite{cun1990, graves2011} используется метод прореживания нейросети. Он заключается в построении заведомо переусложнённой модели, которая в последствии упрощается. Ещё одиним способом, предложенным в работе~\cite{Maclaurin:2015:GHO:3045118.3045343}, является метаобучение, которое получая на вход некоторую выборку возвращает оптимальные гиперпараметры.
	В данной работе исследуется метод, который оптимизирует параметры, гиперпараметры, структурные параметры нейросети в единой процедуре. В основе метода лежит алгоритм DARTS, предложенным в работе~\cite{liu2018darts}. В его основе лежит процедура релаксации: переход от дискретного множества структурных параметров к непрерывному, что позволяет использовать методы градиентной оптимизации для нахождения лучших гиперпараметров. Входными данными метода являются некоторый набор данных и заранее определённый набор функций активации. Как результат мы получаем оптимальную нейросеть.
	
	Проверка и анализ метода проводится на выборке Boston Housing~\cite{Boston}, MNIST~\cite{MNIST},  CIFAR-10~\cite{CIFAR-10} и синтетических данных. Полученная модель сравнивается с моделями, полученными при помощи базовых алгоритмов.
	 

\bibliographystyle{unsrt}
\bibliography{Goryan2018Project11}
\end{document}
