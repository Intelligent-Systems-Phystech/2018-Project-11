\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
\usepackage{footnote}
\usepackage{jmlda}


\begin{document}

\title
    {Автоматическое построение нейросети оптимальной сложности}
\author
    {Горян$^1$~Н.\,А. Бахтеев$^1$~О.\,Ю.  Стрижов$^2$~В.\,В.} % основной список авторов, выводимый в оглавление

\organization
    {$^1$Московский физико-технический институт\par
    $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\email
    {goryan.na@phystech.edu; bakhteev@phystech.edu; strijov@phystech.edu}    


    

\abstract
	{Работа посвящена выбору оптимальной модели нейронной сети. Нейронная сеть рассматривается как вычислительный граф, рёбра которого --- примитивные функции, а вершины --- промежуточные представления выборки. Предполагается, что структуру нейронной сети можно упростить без значимой потери качества классификации. Структура нейросети опеределяется вершинами симплекса. Для определения нужной структуры нейронной сети предлагается проводить оптимизацию гиперпараметров и структурных параметров. Для решения задачи оптимизации предлагается проводить релаксацию структуры. Для анализа качества представленного алгоритма проводятся эксперименты на выборках Boston, MNIST и CIFAR-10.
\bigskip

\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, прореживание нейронной сети, оптимальная структура нейронной сети, вариационный вывод}.
}


\maketitle


\section{ Введение}
	
	В данной работе рассматривается алгоритм построения оптимальной нейронной сети. Одной из основных областей применения являются мобильные устройства, которые в силу своих ограниченных вычислительных ресурсов не могут справляться с избыточно сложными неросетями~\cite{rallapalli2016very}. Существует ряд способов построения нейронных сетей. Для того, чтобы подобрать нужную сеть требуется определить оптимальные значения структурных параметров~\cite{Myung1997}: количество слоёв, нейронов в каждом слое и функции активации каждого нейрона. Выбор этих гиперпараметров является вычислительно сложной задачей~\cite{sutskever2014}.
	
	В работах~\cite{cun1990, graves2011} используется алгоритм прореживания нейросети. Он заключается в построении заведомо переусложнённой модели, которая в последствии упрощается. Ещё одиним способом, предложенным в работе~\cite{Maclaurin:2015:GHO:3045118.3045343}, является метаобучение, которое получая на вход некоторую выборку возвращает оптимальные гиперпараметры.
	В данной работе исследуется алгоритм, который оптимизирует параметры, гиперпараметры, структурные параметры нейросети в единой процедуре. В основе лежит алгоритм DARTS, предложенным в работе~\cite{liu2018darts}, в основе которого лежит процедура релаксации: переход от дискретного множества структурных параметров к непрерывному, что позволяет использовать методы градиентной оптимизации для нахождения лучших гиперпараметров. Входными данными алгоритма являются некоторый набор данных и заранее определённый набор функций активации. Как результат мы получаем оптимальную нейросеть.
	
	Проверка и анализ метода проводится на выборке Boston Housing~\cite{Boston}, MNIST~\cite{MNIST},  CIFAR-10~\cite{CIFAR-10} и синтетических данных. Полученная модель сравнивается с моделями, полученными при помощи базовых алгоритмов.

\section{Постановка задачи}
	Пусть заданы обучающая и валидационная выборки
	$$\mathfrak{D}^{train} = \{\textbf{x}_i,y_i\},~ i =1,...,m^{train},\eqno(?)$$
	$$\mathfrak{D}^{valid} = \{\textbf{x}_i,y_i\},~ i =1,...,m^{valid}, \eqno(?)$$
	где $\mathbf{x}_i \in \mathbb{R}^n$  --- объекты, а $y_{i} \in \{1,..., Z \}$ --- метки объектов $\mathbf{x}_i$, где $Z$ --- количество классов.\\
	Модель описывается ориентированным графом $(V, E)$,  для каждого ребра которого $(j, k) \in E$ определён вектор базовых функций $g_{j, k}$ мощностью $K_{j, k}$. Модель $\textbf{f}(\textbf{x}, \textbf{W})$ задаётся параметрами подмоделей $\{f_{v}\}_{v = 1}^{|V|}$ и структурными параметрами $\gamma$.  
	Каждая подмодель $f_{v}$ описывается через графовое представление модели:
	
	 $$f_{v}(\textbf{x}) = \sum\limits_{k \in Adj(v_i)} <\gamma_{j, k}, g_{j, k}>f_{k}(\textbf{x}, \textbf{w}), \quad f_{0}(\textbf{x}) = \textbf{x}.$$ 
	Параметры модели $\textbf{W}$ --- конкатенация параметров всех подмоделей $\{f_{v}\}_{v = 1}^{|V|}$.\\
	Структура модели $\text{Г}$ --- конкатенация структурных параметров $\gamma$.\\
	\\
	Функции потерь на обучении $L$ и валидации  $Q$ задаются:
	$$L =  loss(\mathfrak{D}^{train}, \textbf{W}, \text{Г})+ e^{A}||\textbf{W}||,$$ 
	$$Q = loss(\mathfrak{D}^{valid}, \textbf{W}, \text{Г}). $$
	Таким образом, получили задачу двухуровневой оптимизации:
	$$W^{*} = \argmin_{ \textbf{W}}L(\mathfrak{D}^{train}, \textbf{W}, \text{Г})$$
	$$\text{Г} = \argmin_{ \text{Г}}Q(\mathfrak{D}^{valid}, \textbf{W}, \text{Г}).$$
	
	
	
  
\bibliographystyle{unsrt}
\bibliography{Goryan2018Project11}
\end{document}
