\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
\usepackage{footnote}
\usepackage{jmlda}

\begin{document}
\title
    {Автоматическое построение нейронной сети оптимальной сложности}
\author
    {Товкес~А.\,A.$^1$, Бахтеев~О.\,Ю.$^1$, Стрижов~В.\,В.$^2$} % основной список авторов, выводимый в оглавление
\email
    {tovkes.aa@phystech.edu;  bakhteev@phystech.edu;  strijov@phystech.edu}
\organization
    {Московский физико-технический институт$^1$;
    	
    Вычислительный центр им. А. А. Дородницына ФИЦ ИУ РАН$^{1, 2}$}
\abstract
	{Работа посвящена задаче выбора оптимальной по сложности модели нейросети. Нейросеть представляется в виде вычислительного графа, где ребрам соответствуют базовые функции, а вершинам --- промежуточные представления выборки под действием этих функций. Параметры сети разделяются на непосредственно параметры модели, которые определяют итоговое качество классификации; гиперпараметры, определяющие процесс обучения и предотвращение переобучения; структурные параметры, определяющие непосредственно структуру модели 
    Для решения задачи оптимизации предлагается проводить релаксацию структуры нейросети. Рассмотрено изменение характеристик нейросети при возмущении структурных параметров. Для анализа качества представленного алгоритма проводятся эксперименты на выборках Boston, MNIST и CIFAR-10.

	

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, оптимальная структура нейронной сети}.

}
\maketitle

\section{1 Введение}
В данной работе рассматривается задача построения оптимальной нейронной сети. В силу большого количества оптимизируемых параметров модели, задача выбора модели глубокого обучения является вычислительно сложной. Поэтому задача выбора структуры модели глубокого обучения включает в себя выбор стратегии параметризации структуры и построения модели, эффективной по вычислительным ресурсам.

Под оптимальной моделью понимается та модель, которая которая дает приемлемое качество при небольшом числе параметров модели. Нейросеть представляется в виде вычислительного графа, где ребрам соответствуют базовые функции, а вершинам --- промежуточные представления выборки под действием этих функций. Сами же структурные параметры —-- это вектор весов каждой базовой функции, определяющий их вклад в итоговую модель.

Существует ряд подходов к построению нейронной сети. В работах~\cite{BrainDamage,PV} предлагается использовать метод прореживания модели. Он заключается в построении переусложненной модели, с последующем удалением параметров, не влияющих на качество. В~\cite{EASNT} используется предсказание структуры модели другой нейросетью. Кроме того в~\cite{GO} используется метаобучение, которое по некторой входной выборке, возвращает оптимальные гиперпараметры.

В данной работе исследуется метод построения модели глубокого обучения, позволяющий производить оптимизацию параметров, гиперпараметров и структурных параметров в единой процедуре. В основе разработанного метода лежит алгоритм DARTS, предложенный в работе~\cite{DARTS}. Для выбора  оптимальных значений гиперпараметров и структурных параметров предлагается  параметризовать структуру
модели некотором действительным вектором, таким образом переходя от дискретного множетсва значений к непрерыному.

Проверка и анализ метода проводится на выборке Boston Housing~\cite{Boston}, MNIST~\cite{MNIST},CIFAR-10~\cite{CIFAR-10} и синтетических данных. Результат сравнивается с моделью полученной при помощи базовых алгоритмов. Критериями качества рассматриваемых алгоритмов выступают качество и полученных моделей и их устойчивость к возмущениям параметров, а также вычилсительная сложность методов.

\section{2 Постановка задачи}
Пусть заданы обучающая и вылидационная выборки:
\[
\mathfrak{D}^{\text{train}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{train}},
\]
\[
\mathfrak{D}^{\text{valid}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{valid}},
\]

состоящие из множеств пар объект-метка,
\[
\mathbf{x}_i\in\mathbf{X}\subset\mathbb{R}^{\text{n}},\quad y_i\in\mathbf{Y}\subset\mathbb{R}.
\] 

$\mathbf{Y}= \{1,\dots,Z\}$, где $Z$ - количество классов.
\\

Модель задаётся ориентированным графом $\mathbf{G=(V,E)}$, где для каждого ребра $(i,j)$ задан вектор базовых функций $\mathbf{g}^{i,j}$, с мощностью $|\mathbf{g}^{i,j}| = K^{i,j}$ и весами $\boldsymbol{\gamma}^{i,j}$. Требуется построить такую модель $\mathbf{f}$ c параметрами $\mathbf{W}\in\mathbb{R}^\text{n}$:
\[
\mathbf{f}(\mathbf{x}, \mathbf{W})= \{ \mathbf{f}_i(\mathbf{x}, \mathbf{w}_i)\}_{i=1}^\mathbf{|V|}
\]

где $\mathbf{f_i(x, w_i)}$ - подмодель c параметрами $\mathbf{w}_i$ задаётся через графовое представление как:
\[
\mathbf{f}_i(\mathbf{x}, \mathbf{w}_i)\ = \sum_{k\in adj(i)} \left\langle {\boldsymbol{\gamma}^{i,k}, \mathbf{g}^{i,k}} \right\rangle \mathbf{f}_k(\mathbf{x}, \mathbf{w}_k)\
\].

Тогда параметры модели --- конкатенация всех параметров каждой подмодели: $\mathbf{W}=[\mathbf{w}_1,\dots,\mathbf{w}_\mathbf{|V|}]$, а структура модели $\boldsymbol{\Gamma}$ задаётся вектором $\{ \boldsymbol{\gamma}^{i,j}\}_\mathbf{E}$.
\\

Функция потерь на обучении $L$ и функция потерь на валидации $Q$ задаются как:
\[
L (\mathbf{W}, \boldsymbol{\Gamma}, \mathbf{A})= \log p(\mathbf{Y}^\text{train}|\mathbf{X}^\text{train}, \mathbf{W}, \boldsymbol{\Gamma}) + \boldsymbol{e}^{\mathbf{A}}||\mathbf{W}||^2,
\]
\[
Q (\mathbf{W}, \boldsymbol{\Gamma})= \log p(\mathbf{Y}^\text{valid}|\mathbf{X}^\text{valid}, \mathbf{W}, \boldsymbol{\Gamma}),
\]
\\

В итоге получаем задачу двухуровневой оптимизации: 
$$\textbf{W}^{*} = \argmin_{ \textbf{W}}L(\mathfrak{D}^{train}, \textbf{W}, \textbf{Г}, \textbf{A})$$
	$$\textbf{Г}^{*} , \textbf{A}^{*} = \argmin_{ \textbf{Г}, \textbf{A}}Q(\mathfrak{D}^{valid}, \textbf{W}, \textbf{Г}, \textbf{A}).$$


\paragraph{Релаксация модели}\\
Для того чтобы более эффективно решать задачу поиска оптимальной структуры нейросети, переходим от дискретной задачи поиска оптимальной базовой функции к непрерывной, производя релаксацию структуры модели используя softmax:\\
$$\overline{g}^{(i, j)}(x) = \sum\limits_{g \in \mathbb{K}}{\frac{exp(\gamma_{g}^{(i, j)})}{\sum\limits_{\overline{g} \in \mathbb{K}}exp(\gamma_{\overline{g}}^{(i, j)})}g(x)},$$\\
 где $\gamma^{(i, j)}$ --- вектор размерности $|\mathbb{K}|$, где $\mathbb{K}$ --- мощность множества кандидатов на роль базовой функции. Этот вектор параметризует комбинацию базовых функций. Таким образом мы перешли к задаче поиска базовой функции, подбирая непрерывные параметры  $\gamma$. В конце поиска, каждая комбинация базовых функций $\overline{g}^{(i, j)}(x) $  меняется на $g^{(i, j)} = \argmax\limits_{g \in \mathbb{K}}\gamma_{g}^{(i, j)}$.\\

 \paragraph{Регуляризация структуры модели}\\
 Регуляризация структуры проводится добавлением к фунции потерь $Q$ слагаемого $\lambda P(\Gamma)$, где $P(\Gamma)$ есть произведение всех вероятностей возникновения веса $\gamma^{(i, j)}$.\\
 Таким образом функция потерь принимает вид:
\[
Q = \text{log p}(\mathbf{Y}^{valid}|\mathbf{X}^{valid}, \mathbf{W}, \boldsymbol{\Gamma}) + \lambda P(\Gamma)
\]
 В качестве вероятности для структуры можно использовать Gumble-Softmax или распределение Дирихле.\\
 
 \paragraph{ Оптимизация гиперпараметров и структурных параметров модели}\\
Потери на валидационной и обучающей выборке обусловлены как структурой модели Г так и параметрами модели W. Цель поиска архитектуры найти модель $\text{Г}^{*}$, которая минимизирует ошибку на валидационной выборке  $Q(w^{*}, \text{Г}^{*})$. При этом $w^{*}$ находится из условия минимизации функции потерь $L(w, \text{Г}^{*}).$ \\
Таким образом получается задача двухуровневой оптимизации:
$$\min\limits_{\text{Г}} \quad L(w^{*}(\text{Г}), \text{Г})$$
$$s.t. \quad w^{*}(\text{Г}) = \argmin_{w}Q(w, \text{Г})$$.\\
Чтобы решить эту задачу мы используем итеративную оптимизационную процедуру, в которой Г и $w$ оптимизируются по очереди с помощью градиентного спуска. На $k$-м шаге, имея структуру модели $\text{Г}_{k-1}$, полуачем $w_{k}$ изменяя $w_{k-1}$ в сторону минимизации $L(w_{k-1}, \text{Г}_{k-1}).$ Далее, фиксируя $w_{k}$, находим $\text{Г}_{k}$, минимизируя $Q(w_{k-1} - \xi \nabla_{w}L(w_{k-1}, \text{Г}_{k-1}), \text{Г}_{k-1}), $  где $\xi$ шаг градиентного спуска.

\bibliographystyle{unsrt}
\bibliography{Tovkes_AA}
\end{document}
