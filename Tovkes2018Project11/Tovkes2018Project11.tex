\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
\usepackage{footnote}
\usepackage{jmlda}

\begin{document}
\title
    {Автоматическое построение нейронной сети оптимальной сложности}
\author
    {Товкес~А.\,A.$^1$, Бахтеев~О.\,Ю.$^1$, Стрижов~В.\,В.$^2$} % основной список авторов, выводимый в оглавление
\email
    {tovkes.aa@phystech.edu;  bakhteev@phystech.edu;  strijov@phystech.edu}
\organization
    {Московский физико-технический институт$^1$;
    	
    Вычислительный центр им. А. А. Дородницына ФИЦ ИУ РАН$^{1, 2}$}
\abstract
	{Работа посвящена задаче выбора оптимальной по сложности модели нейросети. Нейросеть представляется в виде вычислительного графа, где ребрам соответствуют базовые функции, а вершинам --- промежуточные представления выборки под действием этих функций. Параметры сети разделяются на непосредственно параметры модели, которые определяют итоговое качество классификации; гиперпараметры, определяющие процесс обучения и предотвращение переобучения; структурные параметры, определяющие непосредственно структуру модели 
    Для решения задачи оптимизации предлагается проводить релаксацию структуры нейросети. Рассмотрено изменение характеристик нейросети при возмущении структурных параметров. Для анализа качества представленного алгоритма проводятся эксперименты на выборках Boston, MNIST и CIFAR-10.

	

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, оптимальная структура нейронной сети}.

}
\maketitle

\section{1 Введение}
В данной работе рассматривается задача построения оптимальной нейронной сети.В силу большого количества оптимизируемых параметров модели, задача выбора модели глубокого обучения является вычислительно сложной. Поэтому задача выбора структуры модели глубокого обучения включает в себя выбор стратегии параметризации структуры и построения модели, эффективной по вычислительным ресурсам.

Под оптимальной моделью понимается та модель, которая которая дает приемлемое качество при небольшом числе параметров модели. Нейросеть представляется в виде вычислительного графа, где ребрам соответствуют базовые функции, а вершинам --- промежуточные представления выборки под действием этих функций. Сами же структурные параметры —-- это вектор весов каждой базовой функции, определяющий их вклад в итоговую модель.

Существует ряд подходов к построению нейронной сети. В работе~\cite{BrainDamage} предлагается использовать метод прореживания модели. Он заключается в построении переусложненной модели, с последующем удалением параметров, не влияющих на качество. В~\cite{EASNT} используется предсказание структуры модели другой нейросетью. Кроме того в~\cite{GO} используется метаобучение, которое по некторой входной выборке, возвращает оптимальные гиперпараметры.

В данной работе исследуется метод построения модели глубокого обучения, позволяющий производить оптимизацию параметров, гиперпараметров и структурных параметров в единой процедуре. В основе разработанного метода лежит алгоритм DARTS, предложенный в работе~\cite{DARTS}. Для выбора оптимального набора гиперпараметров предлагается  параметризовать структуру
модели некотором действительным вектором, таким образом переходя от дискретного множетсва значений к непрерыному.

Проверка и анализ метода проводится на выборке Boston Housing~\cite{Boston}, MNIST~\cite{MNIST},CIFAR-10~\cite{CIFAR-10} и синтетических данных. Результат сравнивается с моделью полученной при помощи базовых алгоритмов. Критериями качества рассматриваемых алгоритмов выступают качество и полученных моделей и их устойчивость к возмущениям параметров, а также вычилсительная сложность методов.

\section{2 Постановка задачи}
Пусть заданы обучающая и вылидационная выборки:
\[
\mathfrak{D}^{\text{train}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{train}},
\]
\[
\mathfrak{D}^{\text{valid}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{valid}},
\]

состоящие из множеств пар объект-метка,
\[
\mathbf{x}_i\in\mathbf{X}\subset\mathbb{R}^{\text{n}},\quad y_i\in\mathbf{Y}\subset\mathbb{R}.
\] 

$\mathbf{Y}= \{1,\dots,Z\}$, где $Z$ - количество классов.
\\

Модель задаётся ориентированным графом $\mathbf{G=(V,E)}$, где для каждого ребра $(i,j)$ задан вектор базовых функций $\mathbf{g}^{i,j}$, с мощностью $|\mathbf{g}^{i,j}| = K^{i,j}$ и весами $\boldsymbol{\gamma}^{i,j}$. Требуется построить такую модель $\mathbf{f}$ c параметрами $\mathbf{W}\in\mathbb{R}^\text{n}$:
\[
\mathbf{f}(\mathbf{x}, \mathbf{W})= \{ \mathbf{f}_i(\mathbf{x}, \mathbf{w}_i)\}_{i=1}^\mathbf{|V|}
\]

где $\mathbf{f_i(x, w_i)}$ - подмодель c параметрами $\mathbf{w}_i$ задаётся через графовое представление как:
\[
\mathbf{f}_i(\mathbf{x}, \mathbf{w}_i)\ = \sum_{k\in adj(i)} \left\langle {\boldsymbol{\gamma}^{i,k}, \mathbf{g}^{i,k}} \right\rangle \mathbf{f}_k(\mathbf{x}, \mathbf{w}_k)\
\].

Тогда параметры модели --- конкатенация всех параметров каждой подмодели: $\mathbf{W}=[\mathbf{w}_1,\dots,\mathbf{w}_\mathbf{|V|}]$, а структура модели $\boldsymbol{\Gamma}$ задаётся вектором $\{ \boldsymbol{\gamma}^{i,j}\}_\mathbf{E}$.
\\

Функция потерь на обучении $L$ и функция потерь на валидации $Q$ задаются как:
\[
L (\mathbf{W}, \boldsymbol{\Gamma})= \log p(\mathbf{Y}^\text{train}|\mathbf{X}^\text{train}, \mathbf{W}, \boldsymbol{\Gamma}) + \boldsymbol{e}^{\mathbf{A}}||\mathbf{W}||^2,
\]
\[
Q (\mathbf{W}, \boldsymbol{\Gamma})= \log p(\mathbf{Y}^\text{valid}|\mathbf{X}^\text{valid}, \mathbf{W}, \boldsymbol{\Gamma}),
\]
\\

В итоге получаем задачу двухуровневой оптимизации: 
\[
\mathbf{W}^*( \boldsymbol{\Gamma}) = \argmin_{\mathbf{W}}
L (\mathbf{W}, \boldsymbol{\Gamma})\]

\[
\boldsymbol{\Gamma}, \mathbf{A} = \min_{\boldsymbol{\Gamma}} Q (\mathbf{W}^*( \boldsymbol{\Gamma}), \boldsymbol{\Gamma})
\]
\begin{thebibliography}{99}

	\bibitem{BrainDamage}
	\BibAuthor{Yann Le Cun, John S. Denker and Sara A. Solla. }
	\BibTitle{Optimal Brain Damage}. 1989.
		
	\bibitem{EASNT}
	\BibAuthor{Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, Jun Wang. }
	\BibTitle{Efficient Architecture Search by Network Transformation.}. 2017.
		URL: \BibUrl{https://arxiv.org/abs/1707.04873}.


	\bibitem{GO}
	\BibAuthor{D. Maclaurin and D. Duvenaud and R. Adams. }
	\BibTitle{Gradient-based Hyperparameter Optimization Through Reversible Learning} 2015.
	
	\bibitem{DARTS}
	\BibAuthor{Hanxiao~L., Simonyan~K., Yang~.Y. }
	\BibTitle{DARTS: Differentiable Architecture Search}. 2018.
	URL: \BibUrl{https://arxiv.org/abs/1806.09055}.
	
	\bibitem{Boston}
	\BibAuthor{Harrison~Jr. , Rubinfeld~D., Daniel~L. }
	\BibTitle{Hedonic housing prices and the demand for clean air.} 1978.
	URL: \BibUrl{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/}.
	
	\bibitem{MNIST}
	\BibAuthor{Yann LeCun, Corinna Cortes, Christopher J.C. Burges. }
	\BibTitle{The MNIST Database of Handwritten Digits} 1998.
	URL: \BibUrl{http://yann.lecun.com/exdb/mnist/}
	
	\bibitem{CIFAR-10}
	\BibAuthor{A. Krizhevsky, V. Nair, G. Hilton. }
	\BibTitle{The CIFAR-10 dataset} 2009.
	URL: \BibUrl{http://www.cs.toronto.edu/~kriz/cifar.html}

\end{thebibliography}

\end{document}
