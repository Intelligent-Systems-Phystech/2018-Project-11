\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
\usepackage{footnote}
\usepackage{jmlda}

\begin{document}
\title
    {Автоматическое построение нейронной сети оптимальной сложности}
\author
    {Товкес~А.\,A.$^1$, Бахтеев~О.\,Ю.$^1$, Стрижов~В.\,В.$^2$} % основной список авторов, выводимый в оглавление
\email
    {tovkes.aa@phystech.edu;  bakhteev@phystech.edu;  strijov@phystech.edu}
\organization
    {Московский физико-технический институт$^1$;
    	
    Вычислительный центр им. А. А. Дородницына ФИЦ ИУ РАН$^{1, 2}$}
\abstract
	{Работа посвящена задаче выбора оптимальной по сложности модели нейросети. Нейросеть представляется в виде вычислительного графа, где ребрам соответствуют базовые функции, а вершинам --- промежуточные представления выборки под действием этих функций. Параметры сети разделяются на непосредственно параметры модели, которые определяют итоговое качество классификации; гиперпараметры, определяющие процесс обучения и предотвращение переобучения; структурные параметры, определяющие непосредственно структуру модели 
    Для решения задачи оптимизации предлагается проводить релаксацию структуры нейросети. Рассмотрено изменение характеристик нейросети при возмущении структурных параметров. Для анализа качества представленного алгоритма проводятся эксперименты на выборках Boston, MNIST и CIFAR-10.

	

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, оптимальная структура нейронной сети}.

}
\maketitle

\section{1 Введение}
В данной работе рассматривается задача построения оптимальной нейронной сети.В силу высокой вычислительной сложности, время оптимизации нейронных сетей может занимать большое количество времени. Поэтому построение и выбор оптимальной структуры нейронной сети также является вычислительно сложной процедурой, которая значимо влияет на итоговое качество модели.

Под оптимальной моделью понимается та модель, которая дает хорошее качество и имеет максимально простую структуру. Под структурой понимается набор гиперпараметров нейронной сети

Существует ряд подходов к построению нейронной сети. В работе~\cite{BrainDamage} предлагается использовать метод прореживания модели. Он заключается в построении переусложненной модели, с последующем удалением параметров, не влияющих на качество. В~\cite{EASNT} используется предсказание структуры модели другой нейросетью. Кроме того в~\cite{GO} используется метаобучение, которое по некторой входной выборке, возвращает потимальные гиперпараметры.

В данной работе исследуется метод построения нейросети, когда параметры и гиперпараметры оптимизируются одновременно. В основе разработанного метода лежит алгоритм DARTS, предложенный в работе~\cite{DARTS}. Для выбора оптимального набора гиперпараметров предлагается  параметризовать структуру
модели некотором действительным вектором, таким образом переходя от дискретного множетсва значений к непрерыному.

Проверка и анализ метода проводится на выборке Boston Housing~\cite{Boston}, MNIST~\cite{MNIST},CIFAR-10~\cite{CIFAR-10} и синтетических данных. Результат сравнивается с моделzvb полученнsvb при помощи базовых алгоритмов. Оценивается не только качество, но и вычислительная сложность, устойчивость к возмущениям параметров.

\begin{thebibliography}{99}

	\bibitem{BrainDamage}
	\BibAuthor{Yann Le Cun, John S. Denker and Sara A. Solla. }
	\BibTitle{Optimal Brain Damage}. 1989.
		
	\bibitem{EASNT}
	\BibAuthor{Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, Jun Wang. }
	\BibTitle{Efficient Architecture Search by Network Transformation.}. 2017.
		URL: \BibUrl{https://arxiv.org/abs/1707.04873}.


	\bibitem{GO}
	\BibAuthor{D. Maclaurin and D. Duvenaud and R. Adams. }
	\BibTitle{Gradient-based Hyperparameter Optimization Through Reversible Learning} 2015.
	
	\bibitem{DARTS}
	\BibAuthor{Hanxiao~L., Simonyan~K., Yang~.Y. }
	\BibTitle{DARTS: Differentiable Architecture Search}. 2018.
	URL: \BibUrl{https://arxiv.org/abs/1806.09055}.
	
	\bibitem{Boston}
	\BibAuthor{Harrison~Jr. , Rubinfeld~D., Daniel~L. }
	\BibTitle{Hedonic housing prices and the demand for clean air.} 1978.
	URL: \BibUrl{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/}.
	
	\bibitem{MNIST}
	\BibAuthor{Yann LeCun, Corinna Cortes, Christopher J.C. Burges. }
	\BibTitle{The MNIST Database of Handwritten Digits} 1998.
	URL: \BibUrl{http://yann.lecun.com/exdb/mnist/}
	
	\bibitem{CIFAR-10}
	\BibAuthor{A. Krizhevsky, V. Nair, G. Hilton. }
	\BibTitle{The CIFAR-10 dataset} 2009.
	URL: \BibUrl{http://www.cs.toronto.edu/~kriz/cifar.html}

\end{thebibliography}

\end{document}