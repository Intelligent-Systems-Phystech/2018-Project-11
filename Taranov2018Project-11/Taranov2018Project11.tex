\documentclass[12pt, twoside]{article}

\usepackage{jmlda}

\begin{document}

\title{\textsc{Автоматическое определение релевантности параметров нейросети}}

\author
    {Таранов$^1$~С.К. Бахтеев$^1$~О.\,Ю.  Стрижов$^{1,2}$~В.\,В.} 

\email
    {taranov.sk@phystech.edu; bakhteev@phystech.edu; strijov@phystech.edu}
    
\organization
    {$^1$Московский физико-технический институт\par
    $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
	{В данной работе исследуется выбор оптимальной структуры нейронной сети.  Модели нейронных сетей зачастую содержат большое число обучаемых параметров, предполагается, что их число можно снизить с сохранением точности прогноза. Предлагается метод, корректирующий модель в процессе обучения на основе идеи представления сети в виде графа, рёбра которого являются примитивными функциями, а вершины --- промежуточными представленими выборки, полученные под действием этих функций. Для решения задачи оптимизации предлагается проводить релаксацию структуры нейросети, так чтобы соответствующая модель удовлетворяла требованиям точности для данной задачи. Также проводятся численные эксперименты на выборках данных Boston, MNIST, CIFAR-10.

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, релаксация графа}.

}

\maketitle

\section{Введение}
Решается задача построения нейронных сетей оптимальной сложности. Нейронные сети, очень распространённые в текущее время, обладают большой вычислительной сложностью и большим количеством обучаемых параметров. Это усложняет их обучение и использование, особенно на устройствах с ограниченными ресурсами, например, нейронная сеть, построенная по восьмислойной архитектуре AlexNet, имеет около 60 миллионов паратером и требует около 729 MFLOPS$^1$ \cite{Alex_power}. Поэтому для уменьшения сложности и размеров используемых моделей ведутся активные исследования в области оптимизации структур нейронных сетей. 
\footnote[1]{FLOP - единаца измерения работы вычислительной машины, равная одной операцией над числом с плавающей точкой}
В данной работе оптимальными будут считаться структуры, обладающие небольшим числом структурных и обучаемых параметров, при условии сохранения достаточного уровня точности и устойчивости модели. Под структурными параметрами будем понимать те из них, которые описывают структуры модели, например количество нейронных слоёв, количество нейроннов, содержащихся в них, а также функции активации. 

Существует несколько подходов к оптимизации структуры нейронных сетей, один из них --- обработка уже существующих моделей, например, прореживание уже существующих избыточных моделей с последующим дообучением \cite{learn_both, chan_prun, lottery_ticket}. Другой подход к проблеме оптимизации заключается в проектировании новых моделей, являющихся результатом работы алгоритмов на базе обучения с подкреплением \cite{nas}, эволюционных алгоритмов \cite{evol} или как результат релаксации решения оптимизационной задачи \cite{darts}. Основываясь на \cite{darts}, предлагается развить описанный там алгоритм сделав его точнее благодаря использованию вариационного вывода.

Проверка алгоритма произведена на таких выборках как MNIST \cite{mnist} и CIFAR \cite{cifar}, проиведено сравнение с существующими моделями сетей, как полученными в результате других оптимизационных алгоритмов, так и выбранных экспертно на основе эвристических алгоритмов. В качестве критериев качества рассматриваются в первую очередь размер модели и полное время её проектирования и оптимизации, а также точность.

\section{Постановка задачи}
Пусть заданы пространство описания объектов и множество допустимых классов, обозначим их следующим образом:

\[
\mathbb{X}\subset\mathbb{R}^\text{n}, \quad
\mathbb{Y}\subset\mathbb{R} 
\]

Также заданы обучающая и тестовая выборки над декартовым произведением этих пространств, состоящие из описания объекта и класса к которому он принадлежит:
\[
\mathfrak{D}^{\text{train}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{train}},  \quad x \in \mathbb{X}
\]
\[
\mathfrak{D}^{\text{valid}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{valid}},  \quad y \in \mathbb{Y}
\]
Требуется построить модель, которая бы реализовывала эту зависимость. Сформулируем эту задачу как поиск обучаемых параметров модели, минимизирующих функцию потерь $L_\text{train}$ на обучающей выборке и зададим эту функцию потерь следвующим образом:
\[
\mathbf{W}^*( \boldsymbol{\Gamma}) = \argmin_{\mathbf{W}}
L (\mathbf{W}, \boldsymbol{\Gamma}) 
\]
\[
L = \text{log p}(\mathbf{Y}^{train}|\mathbf{X}^{train}, \mathbf{W}, \boldsymbol{\Gamma}) + \boldsymbol{e}^{(\mathbf{A}||\mathbf{W}||^2)} \eqno(1)
\]
где $\mathbf{W}$ - совокупность всех обучаемых параметров, $\boldsymbol{\Gamma}$ - множество всех структурных парметров модели, а $\mathbf{A}$ - регуляризационное слагаемое.

Пусть модель представлена в виде ориентированого, ацикличного графа $\mathbb{G}=(V,E)$, где $E$ - множество всех ребёр графа, а $V$ - множество всех вершин. Как было сказано ранее в вершинах записаны данные, а каждое ребро реализует некоторую функцию, с помощью которой может осуществляется отображение данных из одной вершину в другую, то есть:
\[
\FORALL{g_\text{i,j}} \in E:g_\text{i,j,k} = T(f_k,v_i,V_j), \quad f_k \in F, v_i \in V \quad T(f,v,V) = f(v) \in V 
\]

% \[
% \FORALL{e_\text{i,j}} \in E:e_\text{i,j,\alpha_\text{i,j,k}} = G(f_k,v_i,V_j), \quad f_k \in F, v_i \in E \quad G(f,v,V) = f(v) \in V 
% \]
где $F$ - множество функций, используемых в построении модели, это могут как функции активации так и функции реализующие полносвязные нейронные слои.
Данные в каждой вершине являются линейноё комбинации данных, пришедших от входящих в вершину рёбер.
\[
\FORALL{v_i} \in V:v_i = \sum _{j,k} g_\text{i,j,k} \cdot \gamma _\text{i,j,k}, \quad g_\text{i,j,k} \in E
\]
Коэффициенты этой линейной комбинации $\gamma _\text{i,j,k}$ являются структурными параметрами модели:
\[
\sum _{j,k} \gamma _{i,j,k} = 1, \gamma _{i,j,k} \in \boldsymbol{\Gamma} \quad \mathbf{\gamma} _{j,k} \in [0,1] \eqno(2) 
\]
 Также мы накладываем на структурные параметры ограничение $\eqno(2)$, чтобы в дальнейшем провести релаксацию, приблизив каждую линейную комбинацию одним из её членов. Далее формулируем задачу оптимизации построеной модели, как минимизацию другой функции потерь по пространству структурных параметров. Зададим эту функцию потерь следующем образом
\[
Q = \text{log p}(\mathbf{Y}^{valid}|\mathbf{X}^{valid}, \mathbf{W}, \boldsymbol{\Gamma}) \eqno(3)
\]
Заметим, что пространство структурных параметров $\boldsymbol{\Gamma}$ представляет собой декартово произведение пространств структурных параметров для отдельных вершин, каждое из которых в силу ограничения $\eqno(1)$ является симплексом.
Таким образом итоговую задачу можно сформулировать как задачу двухуровневой оптимизации:
\[
\mathbf{W}^*( \boldsymbol{\Gamma}) = \argmin_{\mathbf{W}}
L (\mathbf{W}, \boldsymbol{\Gamma})\]

\[
\boldsymbol{\Gamma}^*, \mathbf{A}^* = \min_{\boldsymbol{\Gamma}, \mathbf{A}} Q (\mathbf{W}^*( \boldsymbol{\Gamma}), \boldsymbol{\Gamma})
\]

\paragraph{Релаксация модели}

Чтобы эффективно решать задачу поиска потимальной структуры, проводится релаксация модели, с помощью которой дискретной задача поиска архитетектуры оптимизации переводится в непрерывную. Релаксация имеет следующий вид:

\[
\overline{g}^{(i, j)}(x) = \sum\limits_{k: \gamma_{i, j, k} \in \boldsymbol{\Gamma}}{\frac{exp(\gamma_{i, j, k})}{\sum\limits_{k: \gamma_{i, j, k} \in \boldsymbol{\Gamma}}exp(\gamma_^{(i, j, k)})}} g_{i, j, k}(x)
\]
где $\overline{g}^{(i, j)}(x)$ - новая базовая функция отаражающая, которую реализует ребро из $(v_i,v_j)$, после завершения поиска оптимальной структуры, дискретные базовые функции будут выбраны как функции имеющие наибольший вес в неприрывной модели $g^*_{i, j} = g_{i, j, k}$, где $k = \argmax\limits_{k}\gamma_{i, j, k}$

\paragraph{Регуляризация модели}

Для регуляризация структуры модели к фунции потерь $\eqno(3)$ прибавляется специальное слагаемого $\lambda P(\Gamma)$, где $P(\Gamma)$ есть произведение всех вероятностей возникновения веса $\gamma_{i, j, k}$.
Таким образом функция потерь $\eqno(3)$ принимает вид:

\[
Q = \text{log p}(\mathbf{Y}^{valid}|\mathbf{X}^{valid}, \mathbf{W}, \boldsymbol{\Gamma}) + \lambda P(\Gamma)
\]

В качестве этих вероятностей, можно использовать вероятности получаемые из распределений Дирихле и Gumble-Softmax 

\paragraph{Оптимизация обучаемых и структурных парамтеров}

Как видно из функций потерь сформулированных выше, задача оптимизации как обучаемых, так и структурных параметров зависит и как от текущей архитектуры нейронной сети, так и от её текущих весов. Для решения этой двухуровневой оптимизации используется итеративная процедура описанная в \cite{darts}, суть которой заключается в попеременном обновлении обучаемых параметров и структурных. Изменение, которое возникает в нашей задачи - это добавление к структурным параметрам веса возникающие из-за структурных соображений, связанных с уменьшением суммарного описания модели, то есть оптимизации её вида.

\bibliographystyle{unsrt}
\bibliography{Taranov2018Project11}

\end{document}
\[
\boldsymbol{\Gamma}, \mathbf{A} = \min_{\boldsymbol{\Gamma}} Q (\mathbf{W}^*( \boldsymbol{\Gamma}), \boldsymbol{\Gamma})
\]
