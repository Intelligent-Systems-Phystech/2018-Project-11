\documentclass[12pt, twoside]{article}

\usepackage{jmlda}

\begin{document}

\title{\textsc{Автоматическое определение релевантности параметров нейросети}}

\author
    {Таранов$^1$~С.К. Бахтеев$^1$~О.\,Ю.  Стрижов$^{1,2}$~В.\,В.} 

\email
    {taranov.sk@phystech.edu; bakhteev@phystech.edu; strijov@phystech.edu}
    
\organization
    {$^1$Московский физико-технический институт\par
    $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
	{В данной работе исследуется выбор оптимальной структуры нейронной сети.  Модели нейронных сетей зачастую содержат большое число обучаемых параметров, предполагается, что их число можно снизить с сохранением точности прогноза. Предлагается метод, корректирующий модель в процессе обучения на основе идеи представления сети в виде графа, рёбра которого являются примитивными функциями, а вершины --- промежуточными представленими выборки, полученные под действием этих функций. Для решения задачи оптимизации предлагается проводить релаксацию структуры нейросети, так чтобы соответствующая модель удовлетворяла требованиям точности для данной задачи. Также проводятся численные эксперименты на выборках данных Boston, MNIST, CIFAR-10.

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, релаксация графа}.

}

\maketitle

\section{Введение}
Решается задача построения нейронных сетей оптимальной сложности. Нейронные сети, очень распространённые в текущее время, обладают большой вычислительной сложностью и большим количеством обучаемых параметров. Это усложняет их обучение и использование, особенно на устройствах с ограниченными ресурсами. Поэтому для уменьшения сложности и размеров используемых моделей ведутся активные исследования в области оптимизации структур нейронных сетей. 

В данной работе оптимальными будут считаться структуры, обладающие небольшим числом структурных и обучаемых параметров, при условии сохранения достаточного уровня точности и устойчивости модели. Под структурными параметрами будем понимать те из них, которые описывают структуры модели, например количество нейронных слоёв, количество нейроннов, содержащихся в них, а также функции активации. 

Существует несколько подходов к оптимизации структуры нейронных сетей, один из них --- обработка уже существующих моделей, например, прореживание уже существующих избыточных моделей с последующим дообучением \cite{learn_both, chan_prun, lottery_ticket}. Другой подход к проблеме оптимизации заключается в проектировании новых моделей, как результат работы алгоритмов на базе обучения с подкреплением \cite{nas}, эволюционных алгоритмов \cite{evol} или как результат релаксации решения оптимизационной задачи \cite{darts}. Основываясь на \cite{darts}, мы предлагаем развить описанный там алгоритм сделав его точнее благодаря использованию вариационного вывода.

Проверка алгоритма произведена на таких выборках как MNIST \cite{mnist} и CIFAR \cite{cifar}, проиведено сравнение с существующими моделями сетей, как полученными в результате других оптимизационных алгоритмов, так и выбранных экспертно на основе эвристических алгоритмов. В качестве критериев качества рассматриваются в первую очередь размер модели и полное время её проектирования и оптимизации, а также точность.

\section{Постановка задачи}

Постановка нашей задачи состоит из 2 частей - постановки задачи, для которой будет построена модель, и задача оптимизации этой модели. В качестве первой части будем рассматривать постановку задачи классификации объектов. Пусть заданы пространства описания объектов и допустимых классов, обозначим их следующим образом:

\[
\mathbb{X}\subset\mathbb{R}^\text{n}, \quad
\mathbb{Y}\subset\mathbb{R} 
\]

Также заданы обучающая и тестовый выборки над декартовым произведением этих пространств, состоящие соответственно из описания объекта и класса к которому он принадлежит:
\[
\mathfrak{D}^{\text{train}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{train}},
\]
\[
\mathfrak{D}^{\text{valid}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{valid}},
\]
Утверждается, что существует зависимость, сопоставляющая по описанию объекта, соответствующий ему класс, обозначим её как $y^*:\mathbb{X}\xrightarrow{}Y$ Известно что описанные выше выборки удовлетворяют этой зависимости. Требуется построить модель, которая бы реализовывала эту зависимость. Что формулируется как задача поиска обучаемых параметров модели, минимизирующих функцию потерь$L_\text{train}$ на обучающей выборке:
\[
\mathbf{W}^*( \boldsymbol{\Gamma}) = \argmin_{\mathbf{W}}
L_\text{train} (\mathbf{W}, \boldsymbol{\Gamma}) 
\]
\[
L_{train} = \text{log} p(\mathbf{Y}^{train}|\mathbf{X}^{train}, \mathbf{W}, \boldsymbol{\Gamma}) + \boldsymbol{exp}^{(\mathbf{A}||\mathbf{W}||^2)}
\]
где $\mathbb{W}$ - совокупность всех обучаемых параметров, $\boldsymbol{\Gamma}$ - множество всех структурных парметров модели, а $\mathbf{A}$ - регуляризационное слагаемое.

Вторая часть заключается в постановке задачи оптимизации модели, решающий ранее поставленную задачу. Пусть наша модель представлена в виде ориентированого, ацикличного графа $\mathbb{G}=(V,E)$. $E$ - множество всех ребёр графа, каждое из который реализует некоторую функцию, с помощью которой реализуется отображение данных из одной вершину в другую, то есть:
\[
\FORALL{e_\text{i,j}} \in E:e_\text{i,j,k} = G(f_k,v_i,V_j), \quad f_k \in F, v_i \in V \quad G(f,v,V) = f(v) \in V 
\]

% \[
% \FORALL{e_\text{i,j}} \in E:e_\text{i,j,\alpha_\text{i,j,k}} = G(f_k,v_i,V_j), \quad f_k \in F, v_i \in E \quad G(f,v,V) = f(v) \in V 
% \]

где $V$ - множество всех вершин, а каждая вершина $v_i$ характеризуется данными записаными в ней, которые получаются путём линейноё комбинации, пришедших от входящих в вершину рёбер с соответствующими коэффициентами, которые и являются структурными параметрами модели:
\[
\FORALL{v_i} \in V:v_i = \sum _{j,k} e_\text{i,j,k} \cdot \gamma _\text{i,j,k}
\]
\[
\sum _{j,k} \gamma _{i,j,k} = 1, \gamma _{i,j,k} \in \boldsymbol{\Gamma} \eqno(1)
\]
$\mathbb{F}$ - множество функций, используемых в построении модели, это могут как функции активации так и функции реализующие полносвязные нейронные слои, элементы которого $\gamma _{i,j,k} in [0,1]$. Кроме всего прочего мы накладываем ограничение $\eqno(1)$, чтобы в дальнейшем провести релаксацию приблизив линейную комбинацию одним из её членов. Таким образом мы можем сформулировать задачу потимизации построеной модели, как минимизацию следующей функции по пространству структурных гиперпараметров, которое представляет собой декартово произведение симплексов, содержащих структурные параметры для каждой из вершин.
\[
Q = \text{log p}(\mathbf{Y}^{valid}|\mathbf{X}^{valid}, \mathbf{W}, \boldsymbol{\Gamma})
\]
Таким образом итоговую задачу можно сформулировать как задачу двухуровневой оптимизации:
\[
\mathbf{W}^*( \boldsymbol{\Gamma}) = \argmin_{\mathbf{W}}
L (\mathbf{W}, \boldsymbol{\Gamma})\]

\[
\boldsymbol{\Gamma}, \mathbf{A} = \min_{\boldsymbol{\Gamma}} Q (\mathbf{W}^*( \boldsymbol{\Gamma}), \boldsymbol{\Gamma})
\]

\bibliographystyle{unsrt}
\bibliography{Taranov2018Project11}

\end{document}
\[
\boldsymbol{\Gamma}, \mathbf{A} = \min_{\boldsymbol{\Gamma}} Q (\mathbf{W}^*( \boldsymbol{\Gamma}), \boldsymbol{\Gamma})
\]
