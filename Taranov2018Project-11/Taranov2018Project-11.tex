\documentclass[12pt, twoside]{article}

\usepackage{jmlda}

\begin{document}

\title{\textsc{Автоматическое определение релевантности параметров нейросети}}

\author
    {Таранов$^1$~С.К. Бахтеев$^1$~О.\,Ю.  Стрижов$^{1,2}$~В.\,В.} 

\email
    {taranov.sk@phystech.edu; bakhteev@phystech.edu; strijov@phystech.edu}
    
\organization
    {$^1$Московский физико-технический институт\par
    $^2$Вычислительный центр им. А.~А. Дородницына ФИЦ ИУ РАН}

\abstract
	{В данной работе исследуется выбор оптимальной структуры нейронной сети.  Модели нейронных сетей зачастую содержат большое число обучаемых параметров, предполагается, что их число можно снизить с сохранением точности прогноза. Предлагается метод, корректирующий модель в процессе обучения на основе идеи представления сети в виде графа, рёбра которого являются примитивными функциями, а вершины --- промежуточными представленими выборки, полученные под действием этих функций. Для решения задачи оптимизации предлагается проводить релаксацию структуры нейросети, так чтобы соответствующая модель удовлетворяла требованиям точности для данной задачи. Также проводятся численные эксперименты на выборках данных Boston, MNIST, CIFAR-10.

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, релаксация графа}.

}

\maketitle

\section{Введение}
Решается задача создания и анализа эффективного алгоритма построения нейронных сетей оптимальной сложности. Нейронные сети, очень распространённые в текущее время, обладают большой вычислительной сложностью и большим количеством обучаемых параметров. Это усложняет их обучение и использование, особенно на устройствах с ограниченными ресурсами. Поэтому для уменьшения сложности и размеров используемых моделей ведутся активные исследования в области оптимизации структур нейронных сетей. 

В данной работе оптимальными будут считаться структуры, обладающие наименьшим числом гиперпараметров и обучаемых параметров, при условии сохранения достаточного уровня точности и устойчивости модели. Под гиперпараметрами будем понимать те параметры, которые описывают структуры модели, например количество нейронных слоёв, количество нейроннов, содержащихся в них, а также функции активации. 

Существует несколько подходов к оптимизации структуры нейронных сетей, один из них --- обработка уже существующих моделей, например, прореживание уже существующих избыточных моделей с последующим дообучением \cite{learn_both} \cite{chan_prun} \cite{lottery_ticket}. Другой подход к проблеме оптимизации заключается в проектировании новых моделей, как результат работы алгоритмов на базе обучения с подкреплением \cite{nas}, эволюционных алгоритмов \cite{evol} или как результат релаксации решения оптимизационной задачи \cite{darts}. Основываясь на \cite{darts}, мы предлагаем развить описанный там алгоритм сделав его точнее благодаря использованию вариационного вывода.

Проверка алгоритма будет произведена на таких выборках как MNIST \cite{mnist} и CIFAR \cite{cifar}, будет произведено сравнение с существующими моделями сетей, как полученными в результати других оптимизационных алгоритмов, так и построенные людьми. В качестве критериев качества будут рассматриваться в первую очередь размер модели и полное время её проектирования и обучения, а также точность.



\bibliographystyle{unsrt}
\bibliography{Taranov2018Project11}

\end{document}
