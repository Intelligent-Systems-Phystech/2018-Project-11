@Article{Alex_power,
  title =	"Quantized Convolutional Neural Networks for Mobile
		 Devices",
  author =	"Jiaxiang Wu and Cong Leng and Yuhang Wang and Qinghao
		 Hu and Jian Cheng 0001",
  journal =	"CoRR",
  year = 	"2015",
  volume =	"abs/1512.06473",
  bibdate =	"2018-08-13",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1512.html#WuLWHC15",
  URL =  	"http://arxiv.org/abs/1512.06473",
}

@Article{chan_prun,
  title =	"Pruning Filters for Efficient ConvNets",
  author =	"Hao Li and Asim Kadav and Igor Durdanovic and Hanan
		 Samet and Hans Peter Graf",
  journal =	"CoRR",
  year = 	"2016",
  volume =	"abs/1608.08710",
  bibdate =	"2017-06-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1608.html#LiKDSG16",
  URL =  	"http://arxiv.org/abs/1608.08710",
}

@Misc{learn_both,
  title =	"Learning both Weights and Connections for Efficient
		 Neural Networks",
  note = 	"Comment: Published as a conference paper at NIPS
		 2015",
  author =	"Song Han and Jeff Pool and John Tran and William J.
		 Dally",
  year = 	"2015",
  month =	oct # "~30",
  abstract =	"Neural networks are both computationally intensive and
		 memory intensive, making them difficult to deploy on
		 embedded systems. Also, conventional networks fix the
		 architecture before training starts; as a result,
		 training cannot improve the architecture. To address
		 these limitations, we describe a method to reduce the
		 storage and computation required by neural networks by
		 an order of magnitude without affecting their accuracy
		 by learning only the important connections. Our method
		 prunes redundant connections using a three-step method.
		 First, we train the network to learn which connections
		 are important. Next, we prune the unimportant
		 connections. Finally, we retrain the network to fine
		 tune the weights of the remaining connections. On the
		 ImageNet dataset, our method reduced the number of
		 parameters of AlexNet by a factor of 9x, from 61
		 million to 6.7 million, without incurring accuracy
		 loss. Similar experiments with VGG-16 found that the
		 number of parameters can be reduced by 13x, from 138
		 million to 10.3 million, again with no loss of
		 accuracy.",
  bibsource =	"OAI-PMH server at export.arxiv.org",
  oai =  	"oai:arXiv.org:1506.02626",
  subject =	"Computer Science - Neural and Evolutionary Computing;
		 Computer Science - Computer Vision and Pattern
		 Recognition; Computer Science - Learning",
  URL =  	"http://arxiv.org/abs/1506.02626",
}

@Article{lottery_ticket,
  title =	"The Lottery Ticket Hypothesis: Training Pruned Neural
		 Networks",
  author =	"Jonathan Frankle and Michael Carbin",
  journal =	"CoRR",
  year = 	"2018",
  volume =	"abs/1803.03635",
  bibdate =	"2018-04-10",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1803.html#abs-1803-03635",
  URL =  	"http://arxiv.org/abs/1803.03635",
}

@Article{thinet,
  title =	"ThiNet: A Filter Level Pruning Method for Deep Neural
		 Network Compression",
  author =	"Jian-Hao Luo and Jianxin Wu and Weiyao Lin",
  journal =	"CoRR",
  year = 	"2017",
  volume =	"abs/1707.06342",
  bibdate =	"2017-08-05",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1707.html#LuoWL17",
  URL =  	"http://arxiv.org/abs/1707.06342",
}

@Article{nas,
  title =	"Neural Architecture Search with Reinforcement
		 Learning",
  author =	"Barret Zoph and Quoc V. Le",
  journal =	"CoRR",
  year = 	"2016",
  volume =	"abs/1611.01578",
  bibdate =	"2017-06-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1611.html#ZophL16",
  URL =  	"http://arxiv.org/abs/1611.01578",
}

@Article{evol,
  title =	"Large-Scale Evolution of Image Classifiers",
  author =	"Esteban Real and Sherry Moore and Andrew Selle and
		 Saurabh Saxena and Yutaka Leon Suematsu and Quoc V. Le
		 and Alex Kurakin",
  journal =	"CoRR",
  year = 	"2017",
  volume =	"abs/1703.01041",
  bibdate =	"2017-06-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1703.html#RealMSSSLK17",
  URL =  	"http://arxiv.org/abs/1703.01041",
}

@article{darts,
  author    = {Hanxiao Liu and
               Karen Simonyan and
               Yiming Yang},
  title     = {{DARTS:} Differentiable Architecture Search},
  journal   = {CoRR},
  volume    = {abs/1806.09055},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.09055},
  archivePrefix = {arXiv},
  eprint    = {1806.09055},
  timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1806-09055},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{mnist,
  title =	"The MNIST Database of Handwritten Digit Images for
		 Machine Learning Research [Best of the Web]",
  author =	"L. Deng",
  journal =	"IEEE Signal Process. Mag",
  year = 	"2012",
  number =	"6",
  volume =	"29",
  bibdate =	"2017-05-26",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/https://doi.org/10.1109/MSP.2012.2211477;
		 DBLP,
		 http://dblp.uni-trier.de/db/journals/spm/spm29.html#Deng12",
  pages =	"141--142",
}

@book{cifar,
  title =	"Learning Multiple Layers of Features from Tiny Images",
  author =	"Alex Krizhevsky",
  journal =	"IEEE Signal Process. Mag",
  year = 	"2009",
  number =	"4",
  volume =	"8",
  URL =  	"http://arxiv.org/abs/1703.01041",
}
