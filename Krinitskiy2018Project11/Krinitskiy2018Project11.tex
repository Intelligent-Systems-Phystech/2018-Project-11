\documentclass[12pt, twoside]{article}
\usepackage{jmlda}


\title
    {Автоматическое построение нейросети оптимальной сложности }

\author
    {Криницкий~К.\,Д. , Бахтеев~О.\,Ю. , Стрижов~В.\,В.} 
\email
    {krinitskiy.kd@phystech.edu;  bakhteev@phystech.edu;  strijov@phystech.edu}


\abstract{\textbf{Аннотация}:
В этой статье рассматривается задача поиска оптимальной структуры нейронной сети. Нейросеть рассматривается как вычислительный граф, реализуемый с помощью библиотеки Pytorch. Предусматривается, что число структурных параметров можно уменьшить без существенной потери качества классификации или регрессии. Будут изучены изменения характеристик нейронной сети при колебании структурных параметров. Итоговым результатом будет являться модель , дающая приемлемое качество классификации либо регрессии, и не являющаяся избыточной по параметрам.

\bigskip

\

\textbf{Ключевые слова}:  \emph{нейронные сети, графовые вычисления, оптимизация гиперпараметров, вариационный вывод}

}

\begin{document}

\maketitle

\section{Введение}
  В данной работе решается задача построения нейронной сети оптимальной сложности. Под оптимальной моделью имеется ввиду та модель, которая является не избыточной по своим параметрам, но при этом дающая приемлемый результат классификации либо регрессии. В данной статье рассматривается оптимизация структурных параметров, таких как: размерность слоев и их количество, функция активации.
  \par Существует ряд способов выбора модели оптимальной сложности. В работе \cite{GaussianModel} рассматривается модель гауссовского процесса, поясняется как нужно оптимизировать структуру, в случае недостатка информации о входных данных. В \cite{BayesianModel} применяется баейсовская модель, а такаже говорится о принципе $"$Бритва Оккама$"$, который гласит, что из моделей одинаковой точности выбирается наиболее простая. В работах \cite{Gradient1,Gradient2,Gradient3,Gradient4} рассматривается градиентный метод, также являющийся одним из способов оптимизации. 
  \par Построение оптимальной нейронной сети - задача ресурсоемкая и вычислительно трудная. Из-за большого количества структурных параметров время обучение сети сильно возрастает. В данной работе используется эффективный алгоритм, основанный на методе DARTS \cite{DARTS}. Выбор оптимальных значений структурных параметров происходит благодаря процедуре релаксации: переход от дискретного набора параметров к непрерывному.
  \par Проверка полученного алгоритма произведена на данных MNIST \cite{MNIST}, CIFAR-10 \cite{CIFAR}, Boston Housing \cite{Boston} также на синтетических данных. Модели, полученные представленным алгоритмом сравниваются с моделями, построенными с использованием базовых алгоритмов.



\section{2 Постановка задачи}
Рассмотрим для нашей задачи обучающую и валидационную выборки
$$
\mathfrak{D}^{\text{train}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{train}},
$$
$$
\mathfrak{D}^{\text{valid}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{valid}},
$$
$\mathbf{x_i}$ - вектор признаков $i$-го объекта, а $y_i\in\mathbf{Y}\subset\mathbb{R}$, $\mathbf{Y} = \{1,\dots,Z\}$, $Z$ - количество классов.\\
Модель описывается ориентированным графом $(V, E)$. Для каждого ребра $(j, k) \in E$ определён вектор базовых функций $g_{j, k}$ мощностью $K_{j, k}$. Модель $\textbf{f}(\textbf{x}, \textbf{W})$ задаётся параметрами подмоделей $\{f_{v}\}_{v = 1}^{|V|}$ и структурными параметрами $\boldsymbol{\gamma}^{j,k}$.  
	Каждая подмодель $f_{v}$ представляется следующим образом:
	
	 $$f_{v}(\textbf{x}, \textbf{w}_{v}) = \sum\limits_{k \in adj(v_i)} <\gamma_{j, k}, g_{j, k}>f_{k}(\textbf{x}, \textbf{w}_{k}), \quad f_{0}(\textbf{x}) = \textbf{x}.$$ 
	Параметры модели $\mathbf{W}=[\mathbf{w}_1,\dots,\mathbf{w}_\mathbf{|V|}]$ - конкатенация параметров всех подмоделей $\{f_{v}\}_{v = 1}^{|V|}$, а
	структура модели $\boldsymbol{\Gamma}$ - конкатенация структурных параметров $\boldsymbol{\gamma}^{j,k}$.
Пусть $L(\mathfrak{D}^{train},\mathbf{W},\boldsymbol{\Gamma})$ - функция потерь на обучении, а $Q(\mathfrak{D}^{valid},\mathbf{W},\boldsymbol{\Gamma})$ - фунция потерь на валидации. Тогда:
$$L =  \log p(\mathbf{Y}^\text{train}|\mathbf{X}^\text{train}, \mathbf{W}, \boldsymbol{\Gamma}) + \boldsymbol{e}^{\mathbf{A}}||\mathbf{W}||^2$$ 
$$Q =\log p(\mathbf{Y}^\text{valid}|\mathbf{X}^\text{valid}, \mathbf{W}, \boldsymbol{\Gamma}),
$$
$\mathbf{A}$ - гиерпараметр, отвечающий за регуляризацию \\
Гиперпараметры находятся решением двухуровневой задачи оптимизации:
$$\boldsymbol{\Gamma},\mathbf{A} = argmin_{\boldsymbol{\Gamma},\mathbf{A}} Q(\mathfrak{D}^{valid},\mathbf{W^*(\boldsymbol{\Gamma},\mathbf{A})},\boldsymbol{\Gamma},\mathbf{A}),$$
$$\mathbf{W^*} = argmin_W L(\mathfrak{D}^{train},\mathbf{W},\boldsymbol{\Gamma},\mathbf{A})$$


\end{document}


\begin{thebibliography}{99}
    \bibitem{GaussianModel}
	\BibAuthor{Carl E.}
	\BibTitle{Gaussian Processes in Machine Learning}. 2005.
	
	\bibitem{BayesianModel}
	\BibAuthor{David J.C. MacKay}
	\BibTitle{Information Theory, Inference, and Learning Algorithms}. 2005.
	
	\bibitem{Gradient1}
	\BibAuthor{J. Luketina, M. Berglund, T. Raiko, and K. Gref}
	\BibTitle{Scalable gradient-based tuning of continuous
		regularization hyperparameters}. 2016.
		
	\bibitem{Gradient2}
	\BibAuthor{D. Maclaurin, D. Duvenaud, R. P. Adams}
	\BibTitle{Gradient-based Hyperparameter Optimization through Reversible Learning}. 2015.
		
	\bibitem{Gradient3}
	\BibAuthor{L. Franceschi, M. Donini, P. Frasconi, M. Ponti }
	\BibTitle{Forward and Reverse Gradient-Based Hyperparameter Optimization}. 2017.
		
	\bibitem{Gradient4}
	\BibAuthor{Anonymous authors}
	\BibTitle{Online hyper-parameter optimization}. 2018.
		
	\bibitem{DARTS}
	\BibAuthor{Hanxiao~L., Simonyan~K., Yang~.Y}
	\BibTitle{DARTS: Differentiable Architecture Search}. 2018.
	URL: \BibUrl{https://arxiv.org/abs/1806.09055}.
	
	\bibitem{MNIST}
	\BibAuthor{Yann LeCun, Corinna Cortes, Christopher J.C. Burges, }
	\BibTitle{The MNIST Database of Handwritten Digits} 1998.
	URL:{http://yann.lecun.com/exdb/mnist/}
	
	\bibitem{CIFAR}
	\BibAuthor{A. Krizhevsky, V. Nair, G. Hilton. }
	\BibTitle{The CIFAR-10 dataset} 2009.
	URL: \BibUrl{http://www.cs.toronto.edu/~kriz/cifar.html}
	
	\bibitem{Boston}
	\BibAuthor{Harrison~Jr. , Rubinfeld~D., Daniel~L.}
	\BibTitle{Hedonic housing prices and the demand for clean air.} 1978.
	URL:{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/}.
	

\end{thebibliography}




