\documentclass[12pt, twoside]{article}
\usepackage{jmlda}


\title
    {Автоматическое построение нейросети оптимальной сложности }

\author
    {Криницкий~К.\,Д. , Бахтеев~О.\,Ю. , Стрижов~В.\,В.} 
\email
    {krinitskiy.kd@phystech.edu;  bakhteev@phystech.edu;  strijov@phystech.edu}


\abstract{\textbf{Аннотация}:
В этой статье рассматривается задача поиска оптимальной структуры нейронной сети. Нейросеть рассматривается как вычислительный граф, реализуемый с помощью библиотеки Pytorch. Предусматривается, что число структурных параметров можно уменьшить без существенной потери качества классификации или регрессии. Будут изучены изменения характеристик нейронной сети при колебании структурных параметров. Итоговым результатом будет являться модель , дающая приемлемое качество классификации либо регрессии, и не являющаяся избыточной по параметрам.

\bigskip

\

\textbf{Ключевые слова}:  \emph{нейронные сети, графовые вычисления, оптимизация гиперпараметров, вариационный вывод}

}

\begin{document}

\maketitle

\section{Введение}
  В данной работе решается вопрос построения нейронной сети оптимальной сложности. Под оптимальностью имеется ввиду та модель, которая является не избыточной по своим параметрам, но при этом дающаяя относительно приемлемый результат классификации либо регрессии. В данной статье, конкретно, рассматривается оптимизация гиперпараметров, таких как: размерность слоев и их количество, функция активации.
  \par Как известно, существует немало способов выбора модели оптимальной сложности. Гауссовская модель\cite{GaussianModel}, в которой поясняется как нужно оптимизировать структуру, в случае недостатка информации о входных данных. В \cite{BayesianModel}(глава 28) применяется баейсовская модель, а такаже говорится о принципе $"$Бритва Оккама$"$, который гласит, что из моделей одинаковой точности выбирается наиболее простая. В работе \cite{GradientDescent} рассматривается градиент спуск, также являющийся одним из способов оптимизации. 
  \par Построение оптимальной нейронной сети - задача ресурсоемкая и вычислительно трудная. Из-за огромного количества гиперпараметров время обучение сети сильно возрастает. В данной работе используется эффективный, с точки зрения ресурсозатратности, алгоритм, основанный на методе DARTS \cite{DARTS}. Выбор наилучших гиперпараметров происходит благодаря процедуре релаксации: переход от дискретного набора параметров к непрерывному.
  \par Проверка полученного алгоритма произведена на данных MNIST \cite{MNIST}, CIFAR-10 \cite{CIFAR}, Boston Housing \cite{Boston} также на синтетических данных. Результат сравнивается с моделями, построенными на базовых алгоритмах.


\begin{thebibliography}{99}
    \bibitem{GaussianModel}
	\BibAuthor{Carl E.}
	\BibTitle{Gaussian Processes in Machine Learning}. 2005.
	
	\bibitem{BayesianModel}
	\BibAuthor{David J.C. MacKay}
	\BibTitle{Information Theory, Inference, and Learning Algorithms}. 2005.
	
	\bibitem{GradientDescent}
	\BibAuthor{J. Luketina, M. Berglund, T. Raiko, and K. Gref}
	\BibTitle{Scalable gradient-based tuning of continuous
		regularization hyperparameters}. 2016.
		
	\bibitem{DARTS}
	\BibAuthor{Hanxiao~L., Simonyan~K., Yang~.Y}
	\BibTitle{DARTS: Differentiable Architecture Search}. 2018.
	URL: \BibUrl{https://arxiv.org/abs/1806.09055}.
	
	\bibitem{MNIST}
	\BibAuthor{Yann LeCun, Corinna Cortes, Christopher J.C. Burges, }
	\BibTitle{The MNIST Database of Handwritten Digits} 1998.
	URL:{http://yann.lecun.com/exdb/mnist/}
	
	\bibitem{CIFAR}
	\BibAuthor{A. Krizhevsky, V. Nair, G. Hilton. }
	\BibTitle{The CIFAR-10 dataset} 2009.
	URL: \BibUrl{http://www.cs.toronto.edu/~kriz/cifar.html}
	
	\bibitem{Boston}
	\BibAuthor{Harrison~Jr. , Rubinfeld~D., Daniel~L.}
	\BibTitle{Hedonic housing prices and the demand for clean air.} 1978.
	URL:{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/}.
	

\end{thebibliography}


\end{document}



