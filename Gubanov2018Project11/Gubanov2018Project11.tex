\documentclass[12pt,twoside]{article}

\usepackage{jmlda}

\begin{document}
\title
    {Автоматическое построение нейросети оптимальной сложности}
\author
    {Губанов$^1$~С.Е.} % основной список авторов, выводимый в оглавление
\email
    {sergey.gubanov@phystech.edu}
\organization
    {$^1$Московский физико-технический институт}
\abstract
	{Работа посвящена оптимизации структуры нейронной сети. Оптимизация нейронной сети предполагает заданную структуру и значения гиперпараметров. Подобная оптимизация приводит к чрезмерному количеству параметров и неоптимальности структуры, что приводит к невысокой скорости оптимизации и переобучению. В данной работе предлагается новый метод оптимизации, который позволяет учитывать особенности задачи, подстраивая структуру и гиперпараметры в процессе оптимизации. Результатом работы предложенного метода является устойчивая модель, дающая приемлемое качество результатов при меньшей вычислительной сложности.
		
\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, вычислительный граф, прореживание нейронной сети, устойчивость}.

}
\maketitle

\section{Введение}
Современные глубокие нейронные сети являются вычислительно емкими моделями и содержат сотни миллионов параметров~\cite{deepCNN}. Это обуславливает не только длительное время оптимизации, но и ресурсоемкость эксплуатации. Переусложненная модель требует много ресурсов и затрудняет использование в переносимых устройствах и микроконтроллерах. Также существует риск переобучения из-за чрезмерного числа параметров~\cite{overlearning}. Целью данной работы является алгоритм построения нейросети, чтобы эти проблемы, а также проблема устойчивости модели, были учтены. 

Идея автоматического поиска архитектуры нейросети (NAS) известна давно~\cite{NAS1989}, а в современных работах такие алгоритмы показывают сравнимые со state-of-the-art архитектурами результаты ~\cite{zoph2016neural}. Однако, используемая обычно методология оптимизации дискретной структуры нейросети~\cite{deeparchitect}, значительно ограничивает эффективность оптимизаций, не позволяя использовать методы градиентной оптимизации.

Альтернативный подход подразумевает переход от дискретной параметризации структуры нейросети к непрерывной. В работе ~\cite{liu2018darts}, такой переход производится над функциями активации. Затем используется градиентная оптимизация~\cite{gradient}, и выбирается функция с наибольшим весом в каждом отдельном случае.

В данной работе развивается идея релаксации. Оптимизируются не только функции активации, но и остальные структурные параметры нейросети. Предлагается ввести регуляризацию структуры, позволяющую калибровать дискретность параметризации структуры нейросети~\cite{softmax}. При снижении температуры распределение значений структурных параметров приближается к дискретному, что упрощает итоговый выбор структуры нейросети. 

Для оценки полученной системы используются выборки MNIST~\cite{lecun-mnist}, CIFAR-10. Предметом оценки является не только точность ответов на тестовой подвыборке, но и устойчивость результатов.

\bibliography{Gubanov2018Project11}
\bibliographystyle{unsrt}
\end{document}
