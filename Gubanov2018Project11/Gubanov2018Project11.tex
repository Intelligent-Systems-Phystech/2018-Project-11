\documentclass[12pt,twoside]{article}

\usepackage{jmlda}

\begin{document}
\title
    {Автоматическое построение нейросети оптимальной сложности}
\author
    {Губанов$^1$~С.Е.} % основной список авторов, выводимый в оглавление
\email
    {sergey.gubanov@phystech.edu}
\organization
    {$^1$Московский физико-технический институт}
\abstract
	{Работа посвящена оптимизации структуры нейронной сети. Оптимизация нейронной сети предполагает заданную структуру и значения гиперпараметров. Подобная оптимизация приводит к чрезмерному количеству параметров и неоптимальности структуры, что приводит к невысокой скорости оптимизации и переобучению. В данной работе предлагается новый метод оптимизации, который позволяет учитывать особенности задачи, подстраивая структуру и гиперпараметры в процессе оптимизации. Результатом работы предложенного метода является устойчивая модель, дающая приемлемое качество результатов при меньшей вычислительной сложности.
		
\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, вычислительный граф, прореживание нейронной сети, устойчивость}.

}
\maketitle

\section{Введение}
Современные глубокие нейронные сети являются большими вычислительными комплексами и содержат сотни миллионов параметров\cite{deepCNN}. Это обуславливает не только долгое время обучения, но и долгое время применения. Вычислительно сложная обученная модель требует много ресурсов и затрудняет использование в переносимых устройствах и микроконтроллерах. Также существует риск переобучения из-за чрезмерного числа параметров\cite{overlearning}. Целью данной работы является алгоритм построения нейросети, чтобы эти проблемы, а также проблема устойчивости модели, были учтены. 

Идея автоматического поиска архитектуры нейросети (NAS) известна давно\cite{NAS1989}, а в современных работах такие алгоритмы показывают сравнимые со state-of-the-art архитектурами результаты \cite{zoph2016neural}. Однако, используемая обычно методология оптимизации дискретных параметров нейросети\cite{deeparchitect}, значительно ограничивает эффективность оптимизаций, не позволяя использовать методы градиентной оптимизации.

Альтернативный подход подразумевает переход от дискретных параметров нейросети к непрерывным. Например, в работе \cite{liu2018darts}, такой переход производится над функциями активации. Затем используется градиентная оптимизация\cite{gradient}, и выбирается функция с наибольшим весом в каждом отдельном случае.

Суть данной работы состоит в развитии описанной идеи и релаксации не только функции активации, но и остальных гиперпараметров нейросети. Таким образом строится многослойный персептрон с оптимальной структурой. В случае дискретного гиперпараметра использована репараметризация с нормировкой по температуре\cite{softmax}. В процессе обучения температура понижается, и вариант из дискретного множества вариантов гиперпараметра выбирается естественным образом, так как при стремлении температуры к нулю, распределение переходит к категориальному.

Для оценки полученной системы используются стандартные выборки, такие как MNIST\cite{lecun-mnist}, CIFAR-10 и другие. Это дает возможность сравнить результаты с полученными во многих других работах. Предметом оценки является не только точность ответов на тестовой подвыборке, но и устойчивость результатов.

\bibliography{Gubanov2018Project11}
\bibliographystyle{unsrt}
\end{document}
