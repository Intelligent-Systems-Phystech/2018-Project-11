\documentclass[12pt,twoside]{article}

\usepackage{jmlda}

\begin{document}
\title
    {Автоматическое построение нейросети оптимальной сложности}
\author
    {Губанов$^1$~С.Е.} % основной список авторов, выводимый в оглавление
\email
    {sergey.gubanov@phystech.edu}
\organization
    {$^1$Московский физико-технический институт}
\abstract
	{Работа посвящена оптимизации структуры нейронной сети. Оптимизация нейронной сети предполагает заданную структуру и значения гиперпараметров. Подобная оптимизация приводит к чрезмерному количеству параметров и неоптимальности структуры, что приводит к невысокой скорости оптимизации и переобучению. В данной работе предлагается новый метод оптимизации, который позволяет учитывать особенности задачи, подстраивая структуру и гиперпараметры в процессе оптимизации. Результатом работы предложенного метода является устойчивая модель, дающая приемлемое качество результатов при меньшей вычислительной сложности.
		
\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, вычислительный граф, прореживание нейронной сети, устойчивость}.

}
\maketitle

\section{Введение}
Глубокие нейронные сети являются большими вычислительными комплексами. Это обуславливает не только долгое время обучения, но и долгое время применения, а также риск переобучения из-за чрезмерного числа параметров. Целью данной работы является создание алгоритма построения нейросети, чтобы эти проблемы, а также проблема устойчивости модели, были учтены. 

Работы по задаче автоматического поиска архитектуры (NAS) показывают сравнимые со state-of-the-art архитектурами результаты \cite{zoph2016neural}. Однако, используемая обычно методология оптимизации дискретных параметров нейросети, значительно ограничивает эффективность оптимизаций, не позволяя использовать методы градиентной оптимизации.

Альтернативный подход подразумевает переход от дискретных параметров нейросети к непрерывным. Например, в работе \cite{liu2018darts}, такой переход производится над функциями активации. Затем используется градиентная оптимизация, и выбирается функция с наибольшим весом в каждом отдельном случае.

Суть данной работы состоит в развитии описанной идеи и релаксации не только функции активации, но и остальных гиперпараметров нейросети. Таким образом строится многослойный персептрон с оптимальной структурой, функциями активации и связями между узлами.

Для оценки полученной системы используются стандартные выборки, такие как MNIST\cite{lecun-mnist}, CIFAR-10 и другие. Это дает возможность сравнить результаты с полученными во многих других работах. Предметом оценки является не только точность ответов на тестовой подвыборке, но и устойчивость результатов.

\bibliography{Gubanov2018Project11}
\bibliographystyle{unsrt}
\end{document}
