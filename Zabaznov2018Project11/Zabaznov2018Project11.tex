\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    [Автоматическое построение нейросети оптимальной сложности ] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Автоматическое построение нейросети оптимальной сложности. }
\author
    {Забазнов~А.\,Г.$^1$, Бахтеев~О.\,Ю.$^1$, Стрижов~В.\,В.$^{1, 2}$} % основной список авторов, выводимый в оглавление
\email
    {antoniozabaznov@yandex.ru;  bakhteev@phystech.edu;  strijov@phystech.edu}
\organization
    {Московский физико-технический институт$^1$;
    	
    Вычислительный центр им. А. А. Дородницына ФИЦ ИУ РАН$^{1, 2}$}

\abstract
    { В данной работе рассматривается задача выбора оптимальной модели нейросети и оптимизация её параметров. В общем случае нейросеть представляется графом, ребрами которого являются нелинейные операции, а вершины -- промежуточные представления выборки, полученные под действием этих операций. Параметры сети можно разделить на три типа: параметры, отвечающие за итоговое качество классификации; гиперпараметры, отвечающие за процесс обучения и предотвращение переобучения; cтруктурные параметры, отвечающие непосредственно за структуру сети, такие как количество слоев и тип нелинейных операций. Предлагается подход выбора структуры нейросети на основе вариационного вывода  и алгоритма выбора оптимальных значений гиперапараметров  с использованием релаксации, учитывающий неточности при оптимизации параметров и позволяющий находить наиболее устойчивые модели.
    	

\bigskip

\textbf{Ключевые слова}:  \emph{нейронные сети, автоматическое построение нейронных сетей, графовые вычисления, оптимизация параметров нейронной сети, вариационный вывод, оптимальная структура нейронной сети}
}

\begin{document}


\maketitle


\section{Введение}
Выбор оптимальной модели в машинном обучении является одной из ключевых задач.
Под оптимальной моделью понимается структура обучаемой сети и совокупность её гиперпараметров, которые наиболее эффективно решают задачу классификации или регрессии. В данной работе в качестве критерия выбора модели предлагается
сложность модели, то есть будет построена модель, которая не только максимально описывает некоторую выборку, но при этом имеет простую структуры. Под описанием выборки понимается приближенная оценка сложности модели, основанная на связи с её правдоподобием\cite{DescriptionLength}

Существует несколько подходов выбора модели оптимальной сложности. В работе \cite{BrainDamage} используется метод прореживания модели. Он заключается в том, что изначально строится модель со сложным описанием, после обучения которой убираются параметры, которые не влияют на качество решения, таким образом получается сеть наименьшего размера. Ещё одиним способом, предложенным в работе \cite{BayesOptim}, являются байесовские методы оптимизации параметров нейронных сетей. Также в работе\cite{GradientOptim} для оптимизации модели предлагается использовать метод градиентного спуска. 

Задача построения модели оптимальной сложности усложняется тем, что количество параметров в некоторых моделях может достигать большой величины.
Большое число параметров влечет сложность их оптимизации и переобучение
моделей. Кроме того, сам процесс оптимизации становится ресурсоёмким.

В данной работе построение модели оптимальной сложности происходит в процессе самого обучения. В основе разработанного метода лежит алгоритм DARTS, предложенный в работе\cite{DARTS}. Для выбора оптимального набора гиперпараметров предлагается  параметризовать структуру
модели некотором действительным вектором, путём перехода от дискретного множества возможных значений гиперпараметров к непрерывному множетсву их комбинаций.

Проверка и анализ метода проводится на выборке Boston Housing\cite{Boston}, MNIST\cite{MNIST} и синтетических данных. Результат сравнивается с моделью, полученной при помощи базовых алгоритмов.


 

\begin{thebibliography}{99}
	\bibitem{DescriptionLength}
	\BibAuthor{Grunwald~P.}
	\BibTitle{A Tutorial Introduction to the Minimum Description Length Principle}. 2005.
	
	\bibitem{BrainDamage}
	\BibAuthor{Yann Le Cun, John S. Denker and Sara A. Solla.}
	\BibTitle{Optimal Brain Damage}. 1989.
		
	\bibitem{BayesOptim}
	\BibAuthor{A. Neal and M. Radfor}
	\BibTitle{Bayesian Learning for Neural Networks.}. 1995.
	
	\bibitem{GradientOptim}
	\BibAuthor{J. Luketina, M. Berglund, T. Raiko, and K. Gref}
	\BibTitle{Scalable gradient-based tuning of continuous
		regularization hyperparameters.} 2016.
	
	\bibitem{DARTS}
	\BibAuthor{Hanxiao~L., Simonyan~K., Yang~.Y}
	\BibTitle{DARTS: Differentiable Architecture Search}. 2018.
	URL: \BibUrl{https://arxiv.org/abs/1806.09055}.
	
	\bibitem{Boston}
	\BibAuthor{Harrison~Jr. , Rubinfeld~D., Daniel~L.}
	\BibTitle{Hedonic housing prices and the demand for clean air.} 1978.
	URL:{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/}.
	
	\bibitem{MNIST}
	\BibAuthor{Yann LeCun, Corinna Cortes, Christopher J.C. Burges, }
	\BibTitle{The MNIST Database of Handwritten Digits} 1998.
	URL:{http://yann.lecun.com/exdb/mnist/}
		
\end{thebibliography}

\end{document}