\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    [Автоматическое построение нейросети оптимальной сложности ] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Автоматическое построение нейросети оптимальной сложности. }
\author
    {Забазнов~А.\,Г.$^1$, Бахтеев~О.\,Ю.$^1$, Стрижов~В.\,В.$^{1, 2}$} % основной список авторов, выводимый в оглавление
\email
    {antoniozabaznov@yandex.ru;  bakhteev@phystech.edu;  strijov@phystech.edu}
\organization
    {Московский физико-технический институт$^1$;
    	
    Вычислительный центр им. А. А. Дородницына ФИЦ ИУ РАН$^{1, 2}$}

\abstract
    { В данной работе рассматривается задача выбора оптимальной модели нейросети и оптимизация её параметров. В общем случае нейросеть представляется графом, ребрами которого являются нелинейные операции, а вершины -- промежуточные представления выборки, полученные под действием этих операций. Параметры сети можно разделить на три типа: параметры, отвечающие за итоговое качество классификации; гиперпараметры, отвечающие за процесс обучения и предотвращение переобучения; cтруктурные параметры, отвечающие непосредственно за структуру сети, такие как количество слоев и тип нелинейных операций. Предлагается подход выбора структуры нейросети на основе вариационного вывода  и алгоритма выбора оптимальных значений гиперапараметров  с использованием релаксации, учитывающий неточности при оптимизации параметров и позволяющий находить наиболее устойчивые модели.
    	

\bigskip

\textbf{Ключевые слова}:  \emph{нейронные сети, автоматическое построение нейронных сетей, оптимальная структура нейронной сети}
}

\begin{document}


\maketitle


\section{Введение}
При решении задачи классификации или регрессии в машинном обучении выбранная модель зачастую оказывается неоптимальной.
Под оптимальной моделью понимается структура обучаемой сети и совокупность её гиперпараметров, которая даёт приемлемое качество классификации или регрессии при небольшом количестве параметров. В данной работе в качестве критерия выбора модели предлагается сложность модели, то есть величина, учитывающая сложность описания совокупности выборки и модели. Под описанием выборки понимается приближенная оценка сложности модели, основанная на связи с её правдоподобием\cite{DescriptionLength}

Существует несколько подходов выбора модели оптимальной сложности. В работе \cite{BrainDamage} используется метод прореживания модели. Он заключается в построении заведомо переусложнённой модели с дальнейшим удалением параметров, не влияющих на качество классификации, таким образом получается сеть наименьшего размера. Ещё одиним способом, предложенным в работе \cite{BayesOptim}, являются байесовские методы оптимизации параметров нейронных сетей. В работе\cite{GradientOptim} для оптимизации модели предлагается использовать метод градиентного спуска. 

Одна из проблем оптимизации моделей глубокого обучение -- большое количество параметров и гиперпараметров, которое может достигать миллионов. Кроме того, сам процесс оптимизации становится ресурсоёмким. Задача выбора модели глубокого включает в себя выбор стратегии построения модели, эффективной по вычислительным ресурсам. Существуют методы градиентной оптимизации совокупности параметров и гиперпараметров.

В данной работе построение модели оптимальной сложности происходит в процессе самого обучения. В основе разработанного метода лежит алгоритм DARTS, предложенный в работе\cite{DARTS}. Для выбора оптимального набора гиперпараметров предлагается  параметризовать структуру
модели некотором действительным вектором, путём перехода от дискретного множества возможных значений гиперпараметров к непрерывному множетсву их комбинаций.

Проверка и анализ метода проводится на выборке Boston Housing\cite{Boston}, MNIST\cite{MNIST} и  CIFAR-10\cite{CIFAR-10} и синтетических данных. Проводится сравнеине представленного метода с эвристическими алгоритмами выбора модели,  а также с алгоритмом DARTS.

\section{Постановка задачи}

Пусть заданы обучающая и вылидационная выборки:
\[
\mathfrak{D}^{\text{train}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{train}},
\]
\[
\mathfrak{D}^{\text{valid}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{valid}},
\]

состоящие из множеств пар объект-метка,
\[
\mathbf{x}_i\in\mathbf{X}\subset\mathbb{R}^{\text{n}},\quad y_i\in\mathbf{Y}\subset\mathbb{R}.
\] 

Метка $y$ объекта $\mathbf{x}$ принадлежит множеству $y\in\mathbf{Y}= \{1,\dots,Z\}$, где $Z$ - количество классов.
\\

Модель задаётся ориентированным графом $\mathbf{G=(V,E)}$, где для каждого ребра $(i,j)$ заданы базовые функции $\mathbf{g}^{i,j}, |\mathbf{g}^{i,j}| = K^{i,j}$ и их веса $\boldsymbol{\gamma}^{i,j}$. Требуется построить такую модель $\mathbf{f}$ c параметрами $\mathbf{W}\in\mathbb{R}^\text{n}$:
\[
\mathbf{f}(\mathbf{x}, \mathbf{W})= \{ \mathbf{f}_i(\mathbf{x}, \mathbf{w}_i)\}_{i=1}^\mathbf{|V|}
\]

где $\mathbf{f_i(x, w_i)}$ - подмодель c параметрами $\mathbf{w}_i$ задаётся как:
\[
\mathbf{f}_i(\mathbf{x}, \mathbf{w}_i)\ = \sum_{j\in adj(i)} \left\langle {\boldsymbol{\gamma}^{i,j}, \mathbf{g}^{i,j}} \right\rangle \mathbf{f}_j(\mathbf{x}, \mathbf{w}_j)\
\].

Тогда параметры модели определяются как конкатенация всех параметров каждой подмодели: $\mathbf{W}=[\mathbf{w}_1,\dots,\mathbf{w}_\mathbf{|V|}]$, а структура модели $\boldsymbol{\Gamma}$ задаётся вектором $\{ \boldsymbol{\gamma}^{i,j}\}_\mathbf{E}$.
\\

Функция потерь на обучении $L$ и функция потерь на валидации $Q$ задаются как:
\[
L (\mathbf{W}, \boldsymbol{\Gamma})= \log p(\mathbf{Y}^\text{train}|\mathbf{X}^\text{train}, \mathbf{W}, \boldsymbol{\Gamma}) + \boldsymbol{e}^{\mathbf{A}}||\mathbf{W}||^2,
\]
\[
Q (\mathbf{W}, \boldsymbol{\Gamma})= \log p(\mathbf{Y}^\text{valid}|\mathbf{X}^\text{valid}, \mathbf{W}, \boldsymbol{\Gamma}),
\]
где $\mathbf{A}$ - регуляризационное слагаемое. Требуется решить задачу двухуровневой оптимизации, оптимизируя параметры модели по обучающей выборке, а структуру модели по валидационной: 
\[
\mathbf{W}^*( \boldsymbol{\Gamma}) = \argmin_{\mathbf{W}}
L (\mathbf{W}, \boldsymbol{\Gamma})\]

\[
\boldsymbol{\Gamma}, \mathbf{A} = \min_{\boldsymbol{\Gamma}} Q (\mathbf{W}^*( \boldsymbol{\Gamma}), \boldsymbol{\Gamma})
\]

\bibliography{Zabaznov2018Project11.bib}
\bibliographystyle{unsrt}

\end{document}