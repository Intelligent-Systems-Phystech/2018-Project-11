\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    [Автоматическое построение нейросети оптимальной сложности ] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Автоматическое построение нейросети оптимальной сложности. }
\author
    {Забазнов~А.\,Г.$^1$, Бахтеев~О.\,Ю.$^1$, Стрижов~В.\,В.$^{1, 2}$} % основной список авторов, выводимый в оглавление
\email
    {antoniozabaznov@yandex.ru;  bakhteev@phystech.edu;  strijov@phystech.edu}
\organization
    {Московский физико-технический институт$^1$;
    	
    Вычислительный центр им. А. А. Дородницына ФИЦ ИУ РАН$^{1, 2}$}

\abstract
    { В данной работе рассматривается задача выбора оптимальной модели нейросети и оптимизация её параметров. В общем случае нейросеть представляется графом, ребрами которого являются нелинейные операции, а вершины -- промежуточные представления выборки, полученные под действием этих операций. Параметры сети можно разделить на три типа: параметры, отвечающие за итоговое качество классификации; гиперпараметры, отвечающие за процесс обучения и предотвращение переобучения; cтруктурные параметры, отвечающие непосредственно за структуру сети, такие как количество слоев и тип нелинейных операций. Предлагается подход выбора структуры нейросети на основе вариационного вывода  и алгоритма выбора оптимальных значений гиперапараметров  с использованием релаксации, учитывающий неточности при оптимизации параметров и позволяющий находить наиболее устойчивые модели.
    	

\bigskip

\textbf{Ключевые слова}:  \emph{нейронные сети, автоматическое построение нейронных сетей, оптимальная структура нейронной сети}
}

\begin{document}


\maketitle


\section{Введение}
Выбор оптимальной модели в машинном обучении является одной из ключевых задач.
Под оптимальной моделью понимается структура обучаемой сети и совокупность её гиперпараметров, которая даёт приемлемое качество классификации или регрессии при небольшом количестве параметров. В данной работе в качестве критерия выбора модели предлагается сложность модели, то есть величина, учитывающая сложность описания совокупности выборки и модели. Под описанием выборки понимается приближенная оценка сложности модели, основанная на связи с её правдоподобием\cite{DescriptionLength}

Существует несколько подходов выбора модели оптимальной сложности. В работе \cite{BrainDamage} используется метод прореживания модели. Он заключается в построении заведомо переусложнённой модели с дальнейшим удалением параметров, не влияющих на качество классификации, таким образом получается сеть наименьшего размера. Ещё одиним способом, предложенным в работе \cite{BayesOptim}, являются байесовские методы оптимизации параметров нейронных сетей. В работе\cite{GradientOptim} для оптимизации модели предлагается использовать метод градиентного спуска. 

Одна из проблем оптимизации моделей глубокого обучение -- большое количество параметров и гиперпараметров, которое может достигать миллионов. Кроме того, сам процесс оптимизации становится ресурсоёмким. Задача выбора модели глубокого включает в себя выбор стратегии построения модели, эффективной по вычислительным ресурсам. Существуют методы градиентной оптимизации совокупности параметров и гиперпараметров.

В данной работе построение модели оптимальной сложности происходит в процессе самого обучения. В основе разработанного метода лежит алгоритм DARTS, предложенный в работе\cite{DARTS}. Для выбора оптимального набора гиперпараметров предлагается  параметризовать структуру
модели некотором действительным вектором, путём перехода от дискретного множества возможных значений гиперпараметров к непрерывному множетсву их комбинаций.

Проверка и анализ метода проводится на выборке Boston Housing\cite{Boston}, MNIST\cite{MNIST} и синтетических данных. Проводится сравнеине представленного метода с эвристическими алгоритмами выбора модели.


 

\begin{thebibliography}{99}
	\bibitem{DescriptionLength}
	\BibAuthor{Grunwald~P.}
	\BibTitle{A Tutorial Introduction to the Minimum Description Length Principle}. 2005.
	
	\bibitem{BrainDamage}
	\BibAuthor{Yann Le Cun, John S. Denker and Sara A. Solla.}
	\BibTitle{Optimal Brain Damage}. 1989.
		
	\bibitem{BayesOptim}
	\BibAuthor{A. Neal and M. Radfor}
	\BibTitle{Bayesian Learning for Neural Networks.}. 1995.
	
	\bibitem{GradientOptim}
	\BibAuthor{J. Luketina, M. Berglund, T. Raiko, and K. Gref}
	\BibTitle{Scalable gradient-based tuning of continuous
		regularization hyperparameters.} 2016.
	
	\bibitem{DARTS}
	\BibAuthor{Hanxiao~L., Simonyan~K., Yang~.Y}
	\BibTitle{DARTS: Differentiable Architecture Search}. 2018.
	URL: \BibUrl{https://arxiv.org/abs/1806.09055}.
	
	\bibitem{Boston}
	\BibAuthor{Harrison~Jr. , Rubinfeld~D., Daniel~L.}
	\BibTitle{Hedonic housing prices and the demand for clean air.} 1978.
	URL:{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/}.
	
	\bibitem{MNIST}
	\BibAuthor{Yann LeCun, Corinna Cortes, Christopher J.C. Burges, }
	\BibTitle{The MNIST Database of Handwritten Digits} 1998.
	URL:{http://yann.lecun.com/exdb/mnist/}
		
\end{thebibliography}

\end{document}
