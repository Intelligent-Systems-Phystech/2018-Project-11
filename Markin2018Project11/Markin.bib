% Encoding: UTF-8

@book{bishop2006,
 	author = {C. Bishop},
 	title = {Pattern Recognition and Machine Learning},
 	publisher = {Springer},
 	year = {2006},
 	isbn = {0387310738},
 	language =	"eng",
 	address = {Berlin, Heidelberg},
 	oai =  	"oai:cds.cern.ch:998831",
 	subject =	"Computing and Computers",
} 
@InProceedings{pmlr-v37-maclaurin15,
  title = 	 {Gradient-based Hyperparameter Optimization through Reversible Learning},
  author = 	 {Dougal Maclaurin and David Duvenaud and Ryan Adams},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2113--2122},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/maclaurin15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/maclaurin15.html},
  abstract = 	 {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.}
}

@book{MacKay:2002:ITI:971143,
 author = {MacKay, David J. C.},
 title = {Information Theory, Inference \& Learning Algorithms},
 year = {2002},
 isbn = {0521642981},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 

@Inbook{Rasmussen2004,
author="Rasmussen, Carl Edward",
editor="Bousquet, Olivier
and von Luxburg, Ulrike
and R{\"a}tsch, Gunnar",
title="Gaussian Processes in Machine Learning",
bookTitle="Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany, August 4 - 16, 2003, Revised Lectures",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="63--71",
abstract="We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.",
isbn="978-3-540-28650-9",
doi="10.1007/978-3-540-28650-9_4",
url="https://doi.org/10.1007/978-3-540-28650-9_4"
}

@Article{Myung1997,
author="Myung, In Jae
and Pitt, Mark A.",
title="Applying Occam's razor in modeling cognition: A Bayesian approach",
journal="Psychonomic Bulletin {\&} Review",
year="1997",
month="Mar",
day="01",
volume="4",
number="1",
pages="79--95",
abstract="In mathematical modeling of cognition, it is important to have well-justified criteria for choosing among differing explanations (i.e., models) of observed data. This paper introduces a Bayesian model selection approach that formalizes Occam's razor, choosing the simplest model that describes the data well. The choice of a model is carried out by taking into account not only the traditional model selection criteria (i.e., a model's fit to the data and the number of parameters) but also the extension of the parameter space, and, most importantly, the functional form of the model (i.e., the way in which the parameters are combined in the model's equation). An advantage of the approach is that it can be applied to the comparison of non-nested models as well as nested ones. Application examples are presented and implications of the results for evaluating models of cognition are discussed.",
issn="1531-5320",
doi="10.3758/BF03210778",
url="https://doi.org/10.3758/BF03210778"
}

@INPROCEEDINGS{Tishby99theinformation,
    author = {Naftali Tishby and Fernando C. Pereira and William Bialek},
    title = {The Information Bottleneck Method},
    booktitle = {},
    year = {1999},
    pages = {368--377}
}

@article{DBLP:journals/corr/AlemiFD016,
  author    = {Alexander A. Alemi and
               Ian Fischer and
               Joshua V. Dillon and
               Kevin Murphy},
  title     = {Deep Variational Information Bottleneck},
  journal   = {CoRR},
  volume    = {abs/1612.00410},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.00410},
  archivePrefix = {arXiv},
  eprint    = {1612.00410},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AlemiFD016},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
maintained by Schloss Dagstuhl LZI at University of Trier	homebrowsesearchabout

@article{DBLP:journals/corr/TishbyZ15,
  author    = {Naftali Tishby and
               Noga Zaslavsky},
  title     = {Deep Learning and the Information Bottleneck Principle},
  journal   = {CoRR},
  volume    = {abs/1503.02406},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.02406},
  archivePrefix = {arXiv},
  eprint    = {1503.02406},
  timestamp = {Mon, 13 Aug 2018 16:48:20 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TishbyZ15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Shwartz-ZivT17,
  author    = {Ravid Shwartz{-}Ziv and
               Naftali Tishby},
  title     = {Opening the Black Box of Deep Neural Networks via Information},
  journal   = {CoRR},
  volume    = {abs/1703.00810},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.00810},
  archivePrefix = {arXiv},
  eprint    = {1703.00810},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Shwartz-ZivT17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}


@InProceedings{pmlr-v70-franceschi17a,
  title = 	 {Forward and Reverse Gradient-Based Hyperparameter Optimization},
  author = 	 {Luca Franceschi and Michele Donini and Paolo Frasconi and Massimiliano Pontil},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1165--1173},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/franceschi17a/franceschi17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/franceschi17a.html},
  abstract = 	 {We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al (2015) but does not require reversible dynamics. Additionally, we explore the use of constraints on the hyperparameters. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speedup hyperparameter optimization on large datasets. We present a series of experiments on image and phone classification tasks. In the second task, previous gradient-based approaches are prohibitive. We show that our real-time algorithm yields state-of-the-art results in affordable time.}
}

@article{Pedregosa,
 author = {Pedregosa, Fabian},
 title = {Hyperparameter Optimization with Approximate Gradient},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 series = {ICML'16},
 year = {2016},
 location = {New York, NY, USA},
 pages = {737--746},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3045390.3045469},
 acmid = {3045469},
 publisher = {JMLR.org},
}

@incollection{Cun:1990:OBD:109230.109298,
 author = {Cun, Yann Le and Denker, John S. and Solla, Sara A.},
 chapter = {Optimal Brain Damage},
 title = {Advances in Neural Information Processing Systems 2},
 editor = {Touretzky, David S.},
 year = {1990},
 isbn = {1-55860-100-7},
 pages = {598--605},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=109230.109298},
 acmid = {109298},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 


@inproceedings{Sutskever:2014:SSL:2969033.2969173,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
 title = {Sequence to Sequence Learning with Neural Networks},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
 acmid = {2969173},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{Liu2018DARTSDA,
  title={DARTS: Differentiable Architecture Search},
  author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
  journal={CoRR},
  year={2018},
  volume={abs/1806.09055}
}

@inproceedings{DBLP:conf/emnlp/Kim14,
  author    = {Yoon Kim},
  title     = {Convolutional Neural Networks for Sentence Classification},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
               {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  pages     = {1746--1751},
  year      = {2014},
  crossref  = {DBLP:conf/emnlp/2014},
  url       = {http://aclweb.org/anthology/D/D14/D14-1181.pdf},
  timestamp = {Sat, 15 Nov 2014 14:45:18 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/emnlp/Kim14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
