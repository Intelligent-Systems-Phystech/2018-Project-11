\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
%\usepackage[russian]{babel}
\usepackage{jmlda}


\title{Автоматическое построение нейросети оптимальной сложности}
\author{Маркин~В.\,О., Бахтеев~О.\,Ю., Стрижов~В.\,В.}
\date{Октябрь 2018}

\email
    {markin1198@mail.ru}


\organization
    {Московский физико-технический институт}
\abstract
	{В работе рассматривается задача построения оптимальной структуры нейронной сети и исследуется вопрос устойчивости построенной модели. Для оптимизации структурных параметров используется  переход от выбора конкретной архитектуры к выбору комбинации различных архитектур сети и вариационный подход. Также исследуется влияние изменения данных на структуру сети. Для оценки качества и устойчивости моделей, построенных при помощи данного метода, проводятся эксперименты на выборке Boston, MNIST и синтетических данных. Проводится сравнение предложенного алгоритма с другими методами поиска оптимальных моделей нейронной сети.

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, устойчивость нейрносетевой модели.}

}






\begin{document}
\maketitle
\section{1 Ведение}
При использовании нейростетевых моделей в анализе данных часто встает вопрос о выборе архитектуры модели. Нейронная сеть имеет большое число гиперпараметров и долгое время для их настройки использовался перебор и различные эвристические соображения~\cite{DBLP:conf/emnlp/Kim14}. Такой подход вычислеительно неэффективен и не дает гарантий оптимальности полученной модели.


В данной работе рассмтаривается задача построения оптимальной нейрнонной сети.
Под оптимальной моделью понимается модель, дающая приемлемое качество при небольшом количестве параметров. Под структурой нейронной сети понимается набор структурных параметров,хараткетризующих такие величины, как число слоев, размерность каждого слоя, функции активации и параметры регуляризации.
 
 Один из подходов поиска оптимальной структуры --- оптимальное прореживание~\cite{Cun:1990:OBD:109230.109298}, которое заключается в обучении максимально большой сети, при последующем удалении части связей. Другой подход заключается в предсказании структуры модели другой нейросетью~\cite{Sutskever:2014:SSL:2969033.2969173}.
 В данной работе для выбора оптимального набора гиперпараметров проводится процедура релаксации~\cite{Liu2018DARTSDA} --- переход от дискретного множества возможных значений гиперпараметров к непрерывному множетсву их комбинаций. Эта процедура позволяет параметризовать структуру модели некотором действительным вектором.
Такой подход дает возможность применять методы непрерывной оптимизации для нахождения наилучшего набора гиперпараметров.
 Оптимизация гиперпараметров проводится градиентными методами \cite{pmlr-v37-maclaurin15, pmlr-v70-franceschi17a, Pedregosa} либо с использованием Гауссовских процессов и Байесовской оптимизации.



Проводиться вычислительный эксперимент на выборках Boston, MNIST\cite{lecun-mnisthandwrittendigit-2010} и синтетических данных. В ходе экспериментов оценивается не только качество, которое дает полученная модель но и её вычислительная сложноть и устойчивость. Проводится сравнение представленного алгоритма с базовыми алгоритмами построения моделей глубокого обучения.
\section{2 Постановка задачи}
Пусть заданы обущающая и тестовая выборки
$$
\mathfrak{D}^{\text{train}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{train}},
$$
$$
\mathfrak{D}^{\text{valid}} = \{\mathbf{x}_i, y_i\}, \quad i=1,\dots,m^{\text{valid}},
$$
где $\mathbf{x_i}$ - вектор признаков $i$-го объекта, а $y_i$ --- ответ на $i$-ом объекте.\\
Модель описывается ориентированным графом $(V, E)$,  для каждого ребра которого $(j, k) \in E$ определён вектор базовых функций $g_{j, k}$ мощностью $K_{j, k}$. Модель $\textbf{f}(\textbf{x}, \textbf{W})$ задаётся параметрами подмоделей $\{f_{v}\}_{v = 1}^{|V|}$ и структурными параметрами $\gamma$.  
	Каждая подмодель $f_{v}$ описывается через графовое представление модели:
	
	 $$f_{v}(\textbf{x}) = \sum\limits_{k \in Adj(v_i)} <\gamma_{j, k}, g_{j, k}>f_{k}(\textbf{x}, \textbf{w}), \quad f_{0}(\textbf{x}) = \textbf{x}.$$ 
	Параметры модели $\textbf{W}$ --- конкатенация параметров всех подмоделей $\{f_{v}\}_{v = 1}^{|V|}$.\\
	Структура модели $\boldsymbol{\Gamma}$ --- конкатенация структурных параметров $\gamma$.
Пусть $L(\mathfrak{D}^{train},\mathbf{W},\boldsymbol{\Gamma})$ --- функция потерь на обучении, а $Q(\mathfrak{D}^{valid},\mathbf{W},\boldsymbol{\Gamma})$ --- фунция потерь на валидации.
$$L (\mathbf{W}, \boldsymbol{\Gamma})= \log p(\mathbf{y}^\text{train}|\mathbf{X}^\text{train}, \mathbf{W}, \boldsymbol{\Gamma}) + \boldsymbol{e}^{\mathbf{A}}||\mathbf{W}||^2,
$$
$$Q (\mathbf{W}, \boldsymbol{\Gamma})= \log p(\mathbf{Y}^\text{valid}|\mathbf{X}^\text{valid}, \mathbf{W}, \boldsymbol{\Gamma}),
$$
Где $\mathbf{A}$ - гиерпараметр, отвечающий за регуляризацию
гиперпараметры находятся решением двухуровневой задачи оптимизации:
$$\boldsymbol{\Gamma},\mathbf{A} = argmin_{\boldsymbol{\Gamma},\mathbf{A}} Q(\mathfrak{D}^{valid},\mathbf{W^*(\boldsymbol{\Gamma},\mathbf{A})},\boldsymbol{\Gamma},\mathbf{A}),$$
$$\mathbf{W^*} = argmin_W L(\mathfrak{D}^{train},\mathbf{W},\boldsymbol{\Gamma},\mathbf{A})$$
\bibliography{Markin}
\bibliographystyle{unsrt}

\end{document}
