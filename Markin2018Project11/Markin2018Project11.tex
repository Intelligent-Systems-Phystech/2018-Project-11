\documentclass[12pt,twoside]{article}


\usepackage{graphicx}
\usepackage{caption}
%\usepackage[russian]{babel}
\usepackage{jmlda}


\title{Автоматическое построение нейросети оптимальной сложности}
\author{Маркин~В.\,О., Бахтеев~О.\,Ю., Стрижов~В.\,В.}
\date{Октябрь 2018}

\email
    {markin1198@mail.ru}


\organization
    {Московский физико-технический институт}
\abstract
	{В работе рассматривается задача построения оптимальной структуры нейронной сети и исследуется вопрос устойчивости построенной модели. Для оптимизации структурных параметров используется  переход от выбора конкретной архитектуры к выбору комбинации различных архитектур сети и вариационный подход. Также исследуется влияние изменения данных на структуру сети. Для оценки качества и устойчивости моделей, построенных при помощи данного метода, проводятся эксперименты на выборке Boston, MNIST и синтетических данных. Проводится сравнение предложенного алгоритма с другими методами поиска оптимальных моделей нейронной сети.

\bigskip
\textbf{Ключевые слова}: \emph {нейронные сети, оптимизация гиперпараметров, устойчивость нейрносетевой модели.}

}






\begin{document}
\maketitle
\section{1 Ведение}
При использовании нейростетевых моделей в анализе данных часто встает вопрос о выборе архитектуры модели. Нейронная сеть имеет большое число гиперпараметров и долгое время для их настройки использовался перебор и различные эвристические соображения~\cite{DBLP:conf/emnlp/Kim14}. Такой подход вычислеительно неэффективен и не дает гарантий оптимальности полученной модели.


В данной работе рассмтаривается задача построения оптимальной нейрнонной сети.
Под оптимальной моделью понимается не та модель, которая дает максимально хорошее качество, а та, которая еще и имеет максимально простую структуру. Под структурой нейронной сети понимается набор гиперпараметров, таких как число слоев, размерность каждого слоя, функции активации и параметры регуляризации.
 
 Один из подходов поиска оптимальной структуры --- оптимальное прореживание~\cite{Cun:1990:OBD:109230.109298}, которое заключается в обучении максимально большой сети, при последующем удалении части связей. Другой подход заключается в предсказании структуры модели другой нейросетью~\cite{Sutskever:2014:SSL:2969033.2969173}.
 В данной работе для выбора оптимального набора гиперпараметров проводится процедура релаксации~\cite{Liu2018DARTSDA} --- переход от дискретного множества возможных значений гиперпараметров к непрерывному множетсву их комбинаций. Эта процедура позволяет параметризовать структуру модели некотором действительным вектором.
Такой подход дает возможность применять различные методы оптимизации для нахождения наилучшего набора гиперпараметров.
 Оптимизация гиперпараметров проводится градиентными методами \cite{pmlr-v37-maclaurin15, pmlr-v70-franceschi17a, Pedregosa} либо с использованием Гауссовских процессов и Байесовской оптимизации.



Проводиться вычислительный эксперимент на выборках Boston, MNIST\cite{lecun-mnisthandwrittendigit-2010} и синтетических данных. В ходе экспериментов оценивается не только качество, которое дает полученная модель но и её вычислительная сложноть и устойчивость. Также в эксперименте проводится сравнение различных алгоритмов построения оптимальной модели 

\bibliography{Markin}
\bibliographystyle{unsrt}

\end{document}
\end{document}
